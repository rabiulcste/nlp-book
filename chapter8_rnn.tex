\chapter{রিকারেন্ট নিউরাল নেটওয়ার্কস ও ল্যাঙ্গুয়েজ মডেল}
\subsection{n গ্রাম ল্যাঙ্গুয়েজ মডেল}
\subsubsection{ল্যাঙ্গুয়েজ মডেল কি}
ল্যাঙ্গুয়েজ মডেল হচ্ছে পরবর্তী স্ট্রিং প্রেডিকশন টাস্ক। এটাকে ক্লাসিফিকেশন টাস্ক হিশেবে ভাবা যেতে পারে।
\subsubsection{n গ্রামস}
\subsubsection{ল্যাঙ্গুয়েজ মডেল এভালুয়েশন}
\subsubsection{স্মুদিং}

\subsection{রিকারেন্ট নিউরাল নেটস}
রিকারেন্ট নিউরাল নেটওয়ার্কে \cite{6302929} এক ধরণের চেইন থাকে। এটাকে ফিডব্যাক লুপ বলা হয়। চেইন বা সিকুয়েন্সিয়াল বিধায় এই ধরণের নেটয়ার্ক সিকুয়েন্স নিয়ে ভালো কাজ করতে পারে। 
আরএনএন (RNN) এবং মাল্টিলেয়ার পার্সেপ্টন মূলত একই রকম, একমাত্র পার্থক্য হলো আরএনএন এর হিড্ডেন ইউনিটসগুলির মধ্যে কানেকশন রয়েছে।
চিন্তা করা যাক, একটা টেবিলের উপর অনেকগুলি নিউরন একটার একটা এক সারিতে বসানো হলো। এটা হলো আমাদের মাল্টিলেয়ার পার্সেপ্টন।
এই নিউরনগুলির আগেরটির সাথে যদি পরেরটাকে জোড়া দেয়া হয় তবে সেটি হবে রিকারেন্ট নেটওয়ার্ক। 
সাধারণত প্রতি টাইম স্টেপে মডেল আগের টাইম স্টেপের হিড্ডেন স্টেটকে ইনপুট হিশেব নেয়। নেটওয়ার্কে ইনপুট $x$, হিড্ডেন স্টেট $h$ এবং আউটপুট $y$। 
রিকারেন্ট নেটওয়ার্কে টাইম স্টেপ  $t-1$ থেকে $t$ একটা কানেকশন থাকবে। কানেকশন থাকা বলতে বুঝাচ্ছি $t-1$ এর হিড্ডেন ষ্টেট $h_{t-1}$ পরের স্টেপ $t$ এর ইনপুট। 

\begin{align} % requires amsmath; align* for no eq. number
  ￼￼h_t = F(h_{t−1}, \ x_t, \ θ)\\
  h_t = W_{rec} \sigma (h_{t−1}) + U x_t + b \\ 
  y_t = F(V h_t)
\end{align}

নেটওয়ার্ক প্যারামিটার $U, W$ ও $V$ শেয়ারড প্রতি স্টেপে। প্যারামিটার $W$ রিকারেন্ট। ফিডব্যাক লুপের এই প্রোপার্টি ন্যাচারাল ল্যাঙ্গুয়েজের জন্য বেশ কাজের। 
সাধারণত এইযে আমি এখন লিখছি একটা শব্দের পর আরেকটা শব্দের গাঁথুনি হচ্ছে। দেয়াল বানাতে যেমন একটা পর একটা ইটের গাঁথুনি দেয়। 
আমার আগের শব্দ যা লিখলাম তার উপর নির্ভর করে পরের শব্দটী লিখবো। এটা কিন্তু র‍্যান্ডম না। এখানে আগের শব্দের বা শব্দগুচ্ছের (prefix)  একটা গুরুত্বপুর্ণ প্রভাব থাকবে। 
ম্যাশিন লার্নিং বিশেষ করে ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিংয়ে  অধিকাংশ ড্যাটা সিকুয়েন্সিয়াল। টেক্সট, অডিও, ভিডিও ইত্যাদি সবকটা সিকুয়েন্সিয়াল।
রিকারেন্ট নেট এই সিকুয়েন্স ইনফর্মেশনের মাধ্যমে ভালো কন্টেক্সট সংগ্রহ করতে পারে। এই সিকুয়েন্স অর্ডার যদি নেটওয়ার্কে আমরা ইনপুট হিশেবে না পাঠাতে পারি তাহলে 
মডেল ট্রেইন করা সম্ভব হবে না অধিকাংশ ক্ষেত্রে। আশা করি কেন রিকারেন্ট নেটওয়ার্ক আর্কিটেকচার হিশেবে আমরা নির্বাচন করছি তার একটা ব্যাখ্যা দিতে পারলাম। 

\subsection{ট্রেইনিং রিকারেন্ট নিউরাল নেটস}
এবার আমরা নেটওয়ার্ক ট্রেইনিং এর দিকে নজর দিব। এই নেটয়ার্ক আমরা কিভাবে ট্রেইন করতে পারি? এটা অন্যান্য নিউরাল নেট ট্রেইনিং থেকে আলাদা কিছু নয়।
এখানেও আমরা স্টকাস্টিক গ্র্যাডিয়েন্ট ডিসচেন্ট  (SGD) ব্যবহার করবো। রিকারেন্ট নিউরাল নেটের লস হলো প্রতি স্টেপের লসের যোগফল। 
\begin{align} % requires amsmath; align* for no eq. number
   E = \sum_{t=0} ^{T} E_t \\
   \frac {\partial E} {\partial U} = \sum _{t=0} ^{T} \frac {\partial E_t} { \partial U}
\end{align}



\subsubsection{ব্যাকপ্রোপাগেশন থ্রু টাইম (বিপিটিটি)}
আমরা ব্যাকপ্রোপাগেশন ব্যাবহার করবো নিউরাল নেটওয়ার্ক ট্রেইন করার জন্য। বুঝার সুবিধার্থে আমরা এই লস ফাংশন নিয়ে বিস্তারিত চিন্তা করবো এখন। আমরা যদি একটা স্টেপ নিই, $t$ থেকে $t+1$ পজিশনে যাই। তাহলে আমাদের লস ফাংশনের $E$ ডেরিভেটিভ ইনপুট $U$ এর সাপেক্ষে দাঁড়ায়। 

\begin{align} % requires amsmath; align* for no eq. number
   \frac {\partial E_{t+1}} {\partial U} = \frac {\partial E_{t+1}} { \partial h_{t+1}} \frac {\partial h_{t+1}} { \partial U} = \frac {\partial E_{t+1}} { \partial h_{t+1}} \frac {\partial h_{t+1}} { \partial h_t} \frac {\partial h_t} { \partial U}\\
\end{align} 

ব্যাকপ্রোপাগেশন থ্রু টাইম হবে - 
\begin{align} % requires amsmath; align* for no eq. number
   \frac {\partial E_{t+1}} {\partial U} = \sum _{k=0} ^{t+1} \frac {\partial E_{t+1}} { \partial h_{t+1}} \frac {\partial h_{t+1}} { \partial h_k} \frac {\partial h_k} { \partial U} \\
\end{align}



\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=\textwidth]{images/lstm_error.pdf} 
   \caption{রিকারেন্ট নেট ইরর}
   \label{fig:rnn_error}
\end{figure} 


এখানে একটা ট্রিক আমরা ইউজ করবো যার সংক্ষিপ্ত নাম বিপিটিটি \cite{58337}। আমরা যদি লম্বা একটা সিকুয়েন্সের উপর ব্যাকপোপাগেশন করতে যাই সেটা মেমোরি এক্সপেন্সিভ এবং কম্পিউটেশনালি স্লো হতে পারে।
তাই আমরা একটা এপ্রোক্সিমেশন ব্যবহার করবো। 


\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=\textwidth]{images/rnn_derivative.pdf} 
   \caption{রিকারেন্ট নেট ব্যাকপ্রোপাগেশন থ্রু টাইম}
   \label{fig:rnn_derivative}
\end{figure} 

\subsubsection{টিচার ফোর্চিং}
\subsubsection{লং টার্ম ডিপেন্ডেন্সি}
আমরা দেখতে পাচ্ছি রিকারেন্ট নেটের টাইম স্টেপ $t$ তার আগের স্টেপ $t-1$ এর উপর নির্ভরশীল। তাহলে আমাদের সিকুয়েন্স যদি ১০০ টা স্টেপ থাকে তবে সর্বশেষ ধাপ ১০০, তার আগের ধাপ 
৯৯ এর উপর নির্ভরশীল। এখানে আমরা রিকারেন্ট ব্যাপারটা খেয়াল করতে পারি। কারণ ৯৯ তম ধাপ তার আগের ধাপ ৯৮ এর উপর নির্ভরশীল। এভাবে করে ৯৮ ধাপ ৯৭ এর উপর নির্ভশীল এবং 
সবার শেষের ০ তে এসে থামবে। লক্ষ্য করা যাক, ১০০ তম ধাপ চেইন ধরে ৯৯ থেকে ০ সবগুলি ধাপের উপরও নির্ভর করছে। কারণ একটাকে কম্পিউট করতে গেলে তার আগেরটার ইনফরমেশন ছাড়া 
কম্পিউটেশন করা সম্ভব না। এই ধরণের ডিপেন্ডেন্সিকে বলা হয় লং টার্ম ডিপেন্ডেন্সি। 

\subsection{এক্সপ্লোডিং ও ভ্যানিশিং গ্র্যাডিয়েন্ট}
রিকারেন্ট নিউরাল নেট ভ্যানিশিং গ্র্যাডেইয়েন্ট প্রব্লেম হয়। সহজ কথায় বললে, মডেল একটা লম্বা সিকুয়েন্স নিয়ে কাজ করতে গেলে আগে যেসব ইনপুট পেয়েছিল সেসব ভুলে যায়।
ভ্যানিশিং গ্র্যাডিয়েন্টের জন্য আমাদেরকে জ্যাকোভিয়ান টার্মটা বুঝতে হবে। 

\subsubsection{জ্যাকোভিয়ান}
\subsubsection{গ্র্যাডিয়েন্ট ক্লিপিং}

\subsection{লং শর্ট টার্ম মেমোরি নেটওয়ার্ক}
আরএনএন এর ভ্যানিশিং গ্র্যাডিয়েন্ট সমস্যা সমাধান করার জন্য লং শর্ট টার্ম মেমোরি নেটোওয়ার্ক ব্যবহার করা হয়। 
\subsubsection{মেমোরি সেল}
যেকোন কম্পিউটেশনাল মেমোরিতে দুই ধরণের অপারেশন সম্ভবঃ ১) রিড ও ২) রাইট। যদি প্রথমবার লিখা হয় সেটাকে বলা হয় ইনসার্ট আর যদি পুরানো লিখা মুছে (ভুলে গিয়ে) নতুন কিছু লিখা হয় সেটাকে বলা হয় আপডেট। 
 
\subsubsection{ইনপুট, ফরগেট ও আউটপুট গেট}
এলএসটীএম মেমোরি সেলে তথ্য আপডেট করার জন্য ৩ টী গেট ব্যবহার করা হয়। এখানে আমরা ধাপে ধাপে ৩ টী গেটের কাজ বুঝার চেষ্টা করবো। 
 
\begin{align} % requires amsmath; align* for no eq. number
   f_t = \sigma \ (W_f \cdot [H_{t-1} , x_t] \ + \ b_f) \\
   i_t = \sigma \ (W_i \cdot [H_{t-1} , x_t] \ + \ b_i) \\
   \tilde C_t = \tanh \ (W_c \cdot [H_{t-1} , x_t] \ + \ b_c) \\
   C_t = f_t * C_{t-1} + i_t * \tilde C_t \\
   o_t = \sigma \ (W_o \cdot [H_{t-1} , x_t] \ + \ b_o) \\ 
   h_t = o_t * \tanh(C_t) \\ 
\end{align}

\subsection{দুটি অত্যাবশ্যকীয় উপাদানঃ নিউরাল এম্বেডিংস + রিকারেন্ট ল্যাঙ্গুয়েজ মডেল}
আমরা যদি সিকুয়েন্সিয়াল ড্যাটা নিয়ে কাজ করতে চাই তাহলে আমাদের এখন দুটি উপাদান লাগবে- ১) ওয়ার্ড ভেক্টরস বা নিউরাল এমবেডীংস ও ২ ) রিকারেন্ট নিউরাল নেটওয়ার্ক। 
আর এই নেটওয়ার্ক ট্রেইন করতে আমরা ব্যবহার করবো বিপিটিটি এলগরিদম। ওয়ার্ড ভেক্টরস আমাদের ফিচার আর রিকারেন্ট নেট আমাদের ডিপ লার্নিং টুল। 
সাধারণত আমরা এলএসটিএম (LSTM) ব্যবহার করার চেষ্টা করবো কারণ ওর লং টার্ম ডিপেন্ডেন্সি নিয়ে কাজ করার সক্ষমতা বেশি। গ্র্যাডিয়েন্ট ইস্যুর কারণে রিকারেন্ট নেট প্র্যাক্টিক্যালি ট্রেইন করা 
কষ্টসাধ্য কাজ। মোটাদাগে, আধুনিক ভাষা প্রযুক্তির সমস্যা সমাধানে এই দুই অস্ত্র থাকলেই কাজ চালিয়ে নেয়া যাবে! 

\textit{কিছু টিপস\#}  সিকুয়েন্সিয়াল ইনপুট আউটপুট বিষয়ক কিছু সূক্ষ্ম বিষয় আছে যা খেয়াল রাখা জরুরি। আমরা যখন কোড লিখবো বা মডেল পার্ফর্মেন্স এনালাইসিস করবো তখন এই বিষয়গুলি কাজে দিবে।
সিকুয়েন্স মডেলিংয়ের কাজ করতে গেলে আমাদের নিচের কয়েকটি বিষয় মাথায় রাখতে হবে। 

\begin{enumerate}[label=(\roman*)]
\item ভ্যারিয়েবল লেংথ সিকুয়েন্স নিয়ে কাজ করতে হতে পারে (যেকোন টেক্সট ইনপুট;  অথচ ছবি চাইলে আমরা ফিক্সড ডাইমেশন রূপান্তর করে নিতে পারি) 
\item শব্দের অর্ডার মনে রাখা (ভাষার বাক্য গঠন বৈশিষ্ট্য) 
\item লং টার্ম ডিপেন্ডেন্সি মনে রাখা (সমৃদ্ধ কন্টেক্সট এর জন্য দরকার) 
\item সিকুয়েন্সগুলির মধ্যে প্যারামিটার (নেটওয়ার্ক ওয়েটস) শেয়ার করা (ট্রেইনিং এফিশিয়েন্সির জন্য; প্রতি স্টেপে আলাদা প্যারামিটার থাকলে প্যারামিটারের সংখ্যা লিনিয়ারলি বেড়ে যাবে!) 

\end{enumerate}