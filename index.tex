%!TEX program = xelatex

\documentclass{article}[book]
\usepackage{geometry}

 \geometry{
 letterpaper,
 left=1in,
 right=1in,
 top=20mm,
bottom=20mm,
paperheight=11in,
paperwidth=8.5in}

\usepackage{enumitem}
\usepackage{sectsty}

\sectionfont{\fontsize{12}{20\selectfont}

\usepackage{bookmark}  
\usepackage{amsmath}

\usepackage{datetime}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{xcolor}

\usepackage{tcolorbox}

\usepackage{titlesec,titletoc}

%used to import graphics%
\usepackage{graphicx}

%keep only the page number%
%\pagestyle{plain}

% adding fontspec pkg and kalpurush font
\usepackage{fontspec}
\usepackage{polyglossia}
\setdefaultlanguage{bengali}
\setotherlanguages{english}
\newfontfamily{\bengalifont}[Script=Bengali]{Kalpurush}


\title{ডিপ লার্নিং ও ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং}
\author{রবিউল আউয়াল}
\date{১৭ জুলাই, ২০২১}



\begin{document}
\maketitle

\tableofcontents


\section{পরিচিতি }
বাংলায় ন্যাচারাল ল্যাঙ্গুয়েজ  বই লিখার চেষ্টা করছি। 
২০২০ সালের মাঝামাঝি সময়ে আমরা একটা রিডিং গ্রুপ শুরু করি। রিডিং কোর্সের জন্য এই বইটি সহায়ক হিশেবে কাজ শুরু করি।
আমাদের লেকচারগুলি থেকে গুরুত্বপুর্ণ কম্পোনেনট নিয়ে বইটি সাজানো হয়েছে। এতে প্রায় ১৬ টি অধ্যায় রয়েছে। মোট ৪ খণ্ডে বইটি ভাগ করা হয়েছে।   
এই বইকে ডিপ লার্নিং ফর ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং হিসেবে ব্যবহার করা যেতে পারে। 
ন্যাচালার ল্যাঙ্গুয়েজ প্রসেসিং, বাংলায় বলছি ভাষাপ্রযুক্তি। যে টপিকগুলি নিয়ে আলোচনা করা হয়েছে সেগুলির মধ্যে থাকছে - প্রশ্ন উত্তর, ম্যাশিন ট্রান্সলেশন (অনুবাদ) , চ্যাটবট, সিকুয়েন্স ল্যাবেলিং এগুলি হলো জনপ্রিয় সমস্যা। আমার নিজের গবেষণায় খুব একটা উন্নতির আশা দেখছি না। যেহেতু গত দুই বছর ধরে ভাষা প্রযুক্তি নিয়ে পড়ছি। আমি ভাবলাম এই ফিল্ডে যেসকল চমৎকার কাজ হয়েছে, সেগুলি নিয়ে 
আলাপ করতে পারলে কিছুটা মনোবাসনা পূর্ণ হবে। একটা কথা প্রচলিত আছে যেসব লোকেরা নাকি কবিতা লিখতে গিয়ে ব্যর্থ হইছেন তারাই নাকি পরবর্তীতে বাংলা বিভাগের অধ্যক্ষ হন এবং কবিদের ঘাড় মটকানোর 
কাজটি করে বেড়ান পেশাদারিত্বের সাথে। যদিও আমার ইচ্ছে সেরকম কিছু নয়। যাহোক মুগ্ধতার বয়ান আপনাদের সামনে উপস্থাপন করা যাক তবে।

 \part{পার্ট ১} 
\section{ম্যাশিন লার্নিং}
\subsection{সুপারভাইজড লার্নিং সেটআপ}
\subsubsection{ইনপুট, আউটপুট}
\subsubsection{মডেল}
\subsubsection{কস্ট ফাংশন}

\section{ম্যাশিন লার্নিং এলগরিদমস}

\subsection{লজিস্টিক রিগ্রেশন}
\subsection{লিনিয়ার রিগ্রেশন}
\subsection{রেগুলারাইজেশন}

\section{ডীপ লার্নিং}
\subsection{নিউরাল নেটস}
\subsection{ট্রেইনিং নিউরাল নেটস}
\subsection{ব্যাকপ্রোপাগেশন}
\subsection{অপটিমাইজেশন}
\subsection{প্র্যাক্টিক্যাল টিপস ও পাইটর্চ}


 \part{পার্ট ২} 
 
\section{টার্মিনোলজিঃ  হাতি ঘোড়া বাঘ}
\subsection{ল্যাঙ্গুয়েজ}
\subsection{সিকুয়েন্স} 
\subsection{সিমান্টিকস} 
\subsection{সিনট্যাক্স}
\subsection{কনটেক্সট}
\subsection{ফাংশন এপ্রোক্সিমেশন}
\subsection{ল্যাংগুয়েজ এজ ফাংশন এপ্রোক্সিমেশন} 
 
 
\section{ডিস্ট্রিবিউটেড সিমান্টিকস}
\subsection{ডিস্ট্রিবিউটেড ওয়ার্ড রিপ্রেজেন্টেশন}
আমাদের কাছে একটি বড় করপাস আছে। একটা বাক্যের টার্গেট এবং কনটেক্সট এই দুটো টার্ম আমাদের প্রথমে বুঝতে হবে। 
বাক্যগুলিতে  ম্যাশিনের ইনপুট হিশেবে ব্যবহার করার জন্য আমাদের নিউমারিক্যাল ভ্যালুতে রূপান্তর করতে হবে। 
ওয়ান-হট-ভেক্টর। যেখানে একটা ভেক্টরের সবগুলি আইটেমের মান শূন্য শুধু একটা আইটেমের মান ১। ওয়ার্ড টু ভেক্টর হলো একটা টেবিল। 
একে ওয়ার্ড এম্বেডিং বলা হয়। যেকোন বাক্যের এম্বেডিং নিউরাল নেটওয়ার্কের জন্য ইনপুট বা ফিচার হিশেবে কাজ করে।
ওয়ার্ড ভেক্টর অনেক সিমান্টিক তথ্য সংগ্রহ করে রাখতে পারে যা নিউরাল নেটওয়ার্ক ট্রেইনিংয়ে সাহায্য করে।
কন্টিনিউয়াস ব্যাগ অফ ওয়ার্ডস মডেল (সিভোও)। আরেকটি মডেল হচ্ছে স্কিপ গ্রাম মডেল। আমরা যেকোন একটি মডেল ব্যবহার করতে পারি। 
 
 \subsection{নিউরাল ওয়ার্ড এম্বেডিংস}
\subsubsection{ওয়ার্ড ভেকটরস (word2vec) }
ওয়ার্ড টু ভেক হলো আমরা একটা টেবিল চিন্তা করতে পারি। সেই টেবিল থেকে ওয়ার্ড যেখানে ম্যাচ করছে সেই ভ্যালুটা রিটার্ন করবে। 

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.8\textwidth]{images/cbow_skipgram.pdf} 
   \caption{ওয়ার্ড টু ভেক মডেল}
   \label{fig:cbowskipgram}
\end{figure} 


\subsubsection{স্কিপ গ্রাম (Skip-gram)}
\subsubsection{কন্টিনিউয়াস ব্যাগ অব ওয়ার্ডস (CBOW)}
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.8\textwidth]{images/cbow_model.pdf} 
   \caption{কন্টিনিওয়াস ব্যাগ অব ওয়ার্ডস মডেল}
   \label{fig:cbow}
\end{figure} 

\subsubsection{নেগ্যাটিভ স্যামপ্লিং}
নেগেটিভ স্যামপ্লিং হলো পজিটিভ উদাহরণের সাথে নেগেটিভ উদাহরণ ব্যবহার করা। একে কন্ট্রাসটিভ লার্নিং ও বলে। 
\subsection{নিউরাল ওয়ার্ড এম্বেডিংস এভালুয়েশন }

\subsubsection{আউট অব ভোকাবুলারি ওয়ার্ডস ও ক্যারেক্টার কমপোজিশনাল মডেল}

\section{কন্টেক্সুয়াল এম্বেডিংস}
\paragraph{Cove}: 
\paragraph{ELMO}: আমরা ২০১৮ সালে। এখনো সবাই লং শর্ট টার্ম মেমোরি নেটওয়ার্ক ব্যবহার করছে। আমরা দেখেছি কোভে বাইডিরেকশনাল এলএসটিএম ব্যবহার করেছে, ২ লেয়ার। এলমো এসে বলে আমরা আরো গভীর যাবো। কোভ এলএসটিএমের শেষ লেয়ার থেকে হিড্ডেন ষ্টেট নেয়।  এলমো প্রতিটা এলএসটিএম থেকে হিড্ডেন ষ্টেট নেয় এবং সবগুলি মার্জ করে ফেলে। এখানেও আমাদের ওয়ার্ড এম্বেডিংস লাগবে। আমাদের কাছে একটা বড় করপাস থাকবে। সেটার বাক্য আমরা ওয়ার্ডটুভেকে দিব। তারপর সেগুলি ইনপুট হিশেবে পরের লেয়ারে দিব। আগের লেয়ারের আউটপুট আবার পরের লেয়ারে ইনপুট হবে। এভাবে আমরা স্ট্যাক করবো। গ্লোভ হলো নন-কন্টেক্সুয়াল এম্বেডিংস। এলমো হচ্ছে কনটেক্সুয়াল। প্রতিটা এলএসটিএমের হিডেন ষ্টেট জোড়া দিবো। আমরা বাইডিরেকশনাল এলএসটিএম ব্যবহার করবো। একটা ফরোয়ার্ড ল্যাংগুয়েজ মডেল এবং একটা ব্যাকওয়ার্ড ল্যাংগুয়েজ মডেল। ল্যাংগুয়েজ মডেল ব্যবহার করার কারণেই এটা কন্টেক্সুয়াল। কারণ পরের টোকেন প্রেডিক্ট করার জন্য আমরা আগের টোকেনগুলিকে কনটেক্সট হিশেবে ব্যবহার করবো। 


\begin{equation}
\sum_{k=1}^{N} (\log p(t_k|t_1,...,t_{k-1}; \phi_x, \theta_{LSTM}, \theta_s) + \log p(t_k|t_k+1,...,t_{N}; \phi_x, \theta_{LSTM}, \theta_s) )
\end{equation}

যেখানে $\theta_x$ ইনপুট, $\theta_LSTM$ এলএসটিএম আর $\theta_s$ হচ্ছে সফটম্যাক্স প্যারামিটার এবং $N$ হচ্ছে বাক্যের মধ্যে কতগুলি টোকেন আছে। $\log p()$ হচ্ছে ম্যাক্সিমাম লাইকলিহুড। 


\section{সিকুয়েন্স ল্যাবেলিং }
\subsection{সিকুয়েন্স ল্যাবেলিং ও ক্লাসিফিকেশন}
\subsection{সিকুয়েন্স ল্যাবেলিং এপ্লিকেশন}
\subsubsection{পার্টস অফ স্পিচ ট্যাগিং}
\subsubsection{নেইমড এন্টিটি রিকগনিশন}
ল্যাংগুয়েজ প্রোসেসিংয়ে আমরা যে নেটওয়ার্কগুলি ব্যবহার করে থাকি সেগুলি যেকোন একটা ব্যবহার করতে পারি। আমরা ওয়ার্ড এমবেডিং ইউজ করতে পারি, এলএসটিএম ব্যবহার করতে পারি বা কনভনেটস ব্যবহার করতে পারি। সবগুলি নেটোয়ার্কের শেষে আমরা একটা লিনিয়ার লেয়ার ব্যবহার করবো। লিনিয়ার লেয়ারের পরে একটা সফটম্যাক্স প্রেডিকশন হবে। তখন সে আমাকে যতগুলি ট্যাগ আছে সেগুলির আউটপুট দিবে। যদি ৯ টা ক্লাস থাকে সেগুলির থেকে যেকোন আউটপুট হবে। ল্যাংগুয়েজ ইনপুট সিকুয়েন্স চলক দৈর্ঘ্যের হয়ে থাকে। কোন বাক্যের দৈর্ঘ্য হতে পারে ১০, আরেকটি ১২, আরেকটি ৩০। আমরা বাক্যগুলিকে রিশেইপ করে একটি ফিক্সড লেংথে নিয়ে আসবো। রিকারেন্ট নেটওয়ার্ক একটা ফিক্সড লেংথ ইনপুট চায়। আমার বাক্যের দৈর্ঘ্য যদি ১০ হয়, তাহলে বাকি পজিশনগুলিতে আমি ০ দিয়ে দিবো। ও থাকা মানে এই জায়গাগুলিতে অতিরিক্ত প্যাডিং করা হয়েছে। প্যাডিং মানে বাকি ২০ টা টোকেন অতিরিক্ত dummy ও দিয়ে ভরে ফেলা। এখন সিকুয়েন্স প্রেডিকশনের কাজে এই প্যাডিং কাজে দিবে। আমি যে টোকেনগুলি পাচ্ছি, তার যেগুলির মান শূন্য হবে সেগুলির জন্য আমি লস হিশেব করবো না। অই স্থানে মডেলের আউটপুটের জন্য কোন লস কম্পিউট করা হবে না। কারণ যেহেতু ভ্যালু ও তাই এটা আমরা প্যাড করে এনেছি। আপনি যদি মাস্কিং ব্যবহার করেন তাহলে ১০ টা সিকুয়েন্সের জন্য ১০ টা আউটপুত পাবেন এবং লস ১০ টা টোকেনের জন্য আসবে। নিজের কোডিং করতে গেলে এই বিষয়টি মাথায় রাখতে হবে। আপনার নেটওয়ার্ক ফিক্সড লেংথ কিন্তু ইনপুট ভ্যারিয়েবল লেংথ। নেটওয়ার্কগুলি নিজেরা স্মার্ট না তাই এটা এক্সলিসিটলি হ্যান্ডেল করতে হবে যে এখানে আউটপুট হবে না। প্রশ্নঃ যদি ভ্যালু ০ শূন্য হয় তাহলে যদি বাদ দিয়ে দিই, তাহলে মূল সিকুয়েন্সে ০ থাকে? এক্ষেত্রে সচেতন থাকতে হবে যে এম্বেডিং ০ থাকবে না কোন টোকেন এনকোড করার জন্য। যদি এমন হয় এম্বেডিং ০ আছে, তাহলে আমরা -১০০০ দিয়ে মাস্কিং করে নিতে পারি। Glove এমেবডিং ০ নেই, তাই আমরা ০ ব্যবহার করতে পারি। এবার তাহলে রিকারেন্ট নেটওয়ার্ক দিয়ে নেইমড এন্টিটি রিকগনিশন নিয়ে আলাপ করা যাক। 


\begin{comment}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includesvg[width=0.8\textwidth]{/images/rnn_seq} 
   \caption{রিকারেন্ট নেটওয়ার্ক সিকুয়েন্স ট্যাগিং}
   \label{fig:cbow}
\end{figure} 

\end{comment}

আমরা যখন ইমপ্লিমেন্ট করবো তখন আউটপুট লেয়ার থাকবে। BIO ট্যাগ থাকে। B - শুরুর ট্যাগ, I - ইন্টারমিডিয়েট ট্যাগ এবং O - আউটপুট ট্যাগ। এখন আমরা যদি লং শর্ট টার্ম মেমোরি নেটওয়ার্ক ব্যবহার করি তাহলে সমস্যা দেখা দিতে পারি। মাথায় রাখতে হবে আমরা একটি স্ট্রাকচার প্রেডিকশন টাস্ক নিয়ে কাজ করছি। যদি কোন পার্সন শুরু হয় তাহলে পার্সন ট্যাগ ইন্টারমিডিয়েট আসবে বা শেষ হবে। পার্সন শুরু হবার পর অর্গানাইজেশন শুরু বা এন্ড হতে পারবে না। এলএসটিএম কি ধরণের সমস্যায় পড়তে পারে এ ধরণের স্টাকচারড প্রেডিকশন হ্যান্ডল করতে গেলে। দুটি শব্দ দিয়ে একই এন্টিটি রিপ্রেজেন্ট করে তখন কি হবে? আমরা স্ট্রাকচারড কোডিং স্কিম থেকে জানি কোন ট্যাগ শুরু হলে আগে সেই ট্যাগ শেষ হতে হবে। এলএসটিএম এর আগের স্টেপে আউটপুট প্রেডিকশন জানা নেই। আগের টোকেনের প্রেডিকশন জানা নেই, তাই ডিসিশনগুলি সবসময় লোকাল। এটার সমাধান কি হতে পারে? এর সমাধান হতে পারে আউটপুটের মধ্যে ডিপেন্ডেন্সি তইরি করা। আগের স্টেপে যা আউটপুট জেনারেট করলাম সেটা পরের স্টেপকে জানিয়ে দেয়া। এখন প্রশ্ন হচ্ছে আমরা কি ধরণের নেটওয়ার্ক দিয়ে এই ডিপেন্ডেন্সি জেনারেট করতে পারি। হিড্ডেন মার্কভ মডেল জানা থাকলে এই আলাপটা করা খুবই সহজ। মার্কভ প্রোপার্টি মনে করে যে আপনার যদি আগের ষ্টেটের তথ্য থাকে তাহলে পরের ষ্টেট অই ষ্টেটের উপর কন্ডিশন করলে বাকি ষ্টেটগুলি না জানলেও হবে। আমরা এখানে আবহাওয়ার তাপমাত্রা উদাহরণ হিশেবে দেখতে পারি। আমরা যদি গতকালের তাপমাত্রা উপর কন্ডিশন করি তাহলে এর আগে কিছু আমাদের এর আগের তাপমাত্রা জানার প্রয়োজন নেই। আগের ঘটনাগুলির সাপেক্ষে আজকের ঘটনাটি ইন্ডিপেন্ডেন্ট। কারণ হচ্ছে গতকালের ঘটনা আগের ঘটনাগুলির তথ্য ক্যারি করে। কন্ডিশনাল র‍্যান্ডম ফিল্ড লেয়ার ব্যবহার করে এই সমস্যার সমাধান সম্ভব। 

CRF লেয়ার আউটপুট সিকুয়েন্স $y$ এর প্রবাবিলিটি নির্নয় করে নিচের ইকুয়েশন দিয়ে, যেখানে $x$ ইনপুট সিকুয়েন্স। 
 
\begin{equation} % requires amsmath; align* for no eq. number
P(y|x) =  \frac{\exp \phi(x, y)}{\sum_{y^\prime} \exp \phi(x,y^\prime)} \\
= \frac{\exp \phi(x, y)}{Z(x)} \\
\end{equation}
এখানে $\phi(x,y)$ স্কোরিং ফাংশন,  নিউরাল নেটওয়ার্ক। সম্ভাব্যতা হচ্ছে একটা বিশাল বড় সফটম্যাক্স। হরে যে $\sum$ সেটা সকল সিকুয়েন্সের সম্ভাব্যতা হিশেব করে। এই বিশাল $\sum$ কে বলা হয় পার্টিশন ফাংশন $Z(x)$.


\section{লিঙ্গুইস্টিক স্ট্রাকচার}
\subsection{ডিপেন্ডেন্সি পার্সার}
\subsection{ইউনিভার্সাল ডিপেন্ডেন্সিস}

\section{রেফারেন্স রেজ্যুলুশন}

 \part{পার্ট ৩} 
\section{রিকারেন্ট নিউরাল নেটওয়ার্কস ও ল্যাঙ্গুয়েজ মডেল}
\subsection{n গ্রাম ল্যাঙ্গুয়েজ মডেল}
\subsubsection{ল্যাঙ্গুয়েজ মডেল কি}
ল্যাঙ্গুয়েজ মডেল হচ্ছে পরবর্তী স্ট্রিং প্রেডিকশন টাস্ক। এটাকে ক্লাসিফিকেশন টাস্ক হিশেবে ভাবা যেতে পারে।
\subsubsection{n গ্রামস}
\subsubsection{ল্যাঙ্গুয়েজ মডেল এভালুয়েশন}
\subsubsection{স্মুদিং}

\subsection{রিকারেন্ট নিউরাল নেটস}
রিকারেন্ট নিউরাল নেটওয়ার্কে \cite{6302929} এক ধরণের চেইন থাকে। এটাকে ফিডব্যাক লুপ বলা হয়। চেইন বা সিকুয়েন্সিয়াল বিধায় এই ধরণের নেটয়ার্ক সিকুয়েন্স নিয়ে ভালো কাজ করতে পারে। 
আরএনএন (RNN) এবং মাল্টিলেয়ার পার্সেপ্টন মূলত একই রকম, একমাত্র পার্থক্য হলো আরএনএন এর হিড্ডেন ইউনিটসগুলির মধ্যে কানেকশন রয়েছে।
চিন্তা করা যাক, একটা টেবিলের উপর অনেকগুলি নিউরন একটার একটা এক সারিতে বসানো হলো। এটা হলো আমাদের মাল্টিলেয়ার পার্সেপ্টন।
এই নিউরনগুলির আগেরটির সাথে যদি পরেরটাকে জোড়া দেয়া হয় তবে সেটি হবে রিকারেন্ট নেটওয়ার্ক। 
সাধারণত প্রতি টাইম স্টেপে মডেল আগের টাইম স্টেপের হিড্ডেন স্টেটকে ইনপুট হিশেব নেয়। নেটওয়ার্কে ইনপুট $x$, হিড্ডেন স্টেট $h$ এবং আউটপুট $y$। 
রিকারেন্ট নেটওয়ার্কে টাইম স্টেপ  $t-1$ থেকে $t$ একটা কানেকশন থাকবে। কানেকশন থাকা বলতে বুঝাচ্ছি $t-1$ এর হিড্ডেন ষ্টেট $h_{t-1}$ পরের স্টেপ $t$ এর ইনপুট। 

\begin{align} % requires amsmath; align* for no eq. number
  ￼￼h_t = F(h_{t−1}, \ x_t, \ θ)\\
  h_t = W_{rec} \sigma (h_{t−1}) + U x_t + b \\ 
  y_t = F(V h_t)
\end{align}

নেটওয়ার্ক প্যারামিটার $U, W$ ও $V$ শেয়ারড প্রতি স্টেপে। প্যারামিটার $W$ রিকারেন্ট। ফিডব্যাক লুপের এই প্রোপার্টি ন্যাচারাল ল্যাঙ্গুয়েজের জন্য বেশ কাজের। 
সাধারণত এইযে আমি এখন লিখছি একটা শব্দের পর আরেকটা শব্দের গাঁথুনি হচ্ছে। দেয়াল বানাতে যেমন একটা পর একটা ইটের গাঁথুনি দেয়। 
আমার আগের শব্দ যা লিখলাম তার উপর নির্ভর করে পরের শব্দটী লিখবো। এটা কিন্তু র‍্যান্ডম না। এখানে আগের শব্দের বা শব্দগুচ্ছের (prefix)  একটা গুরুত্বপুর্ণ প্রভাব থাকবে। 
ম্যাশিন লার্নিং বিশেষ করে ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিংয়ে  অধিকাংশ ড্যাটা সিকুয়েন্সিয়াল। টেক্সট, অডিও, ভিডিও ইত্যাদি সবকটা সিকুয়েন্সিয়াল।
রিকারেন্ট নেট এই সিকুয়েন্স ইনফর্মেশনের মাধ্যমে ভালো কন্টেক্সট সংগ্রহ করতে পারে। এই সিকুয়েন্স অর্ডার যদি নেটওয়ার্কে আমরা ইনপুট হিশেবে না পাঠাতে পারি তাহলে 
মডেল ট্রেইন করা সম্ভব হবে না অধিকাংশ ক্ষেত্রে। আশা করি কেন রিকারেন্ট নেটওয়ার্ক আর্কিটেকচার হিশেবে আমরা নির্বাচন করছি তার একটা ব্যাখ্যা দিতে পারলাম। 

\subsection{ট্রেইনিং রিকারেন্ট নিউরাল নেটস}
এবার আমরা নেটওয়ার্ক ট্রেইনিং এর দিকে নজর দিব। এই নেটয়ার্ক আমরা কিভাবে ট্রেইন করতে পারি? এটা অন্যান্য নিউরাল নেট ট্রেইনিং থেকে আলাদা কিছু নয়।
এখানেও আমরা স্টকাস্টিক গ্র্যাডিয়েন্ট ডিসচেন্ট  (SGD) ব্যবহার করবো। রিকারেন্ট নিউরাল নেটের লস হলো প্রতি স্টেপের লসের যোগফল। 
\begin{align} % requires amsmath; align* for no eq. number
   E = \sum_{t=0} ^{T} E_t \\
   \frac {\partial E} {\partial U} = \sum _{t=0} ^{T} \frac {\partial E_t} { \partial U}
\end{align}



\subsubsection{ব্যাকপ্রোপাগেশন থ্রু টাইম (বিপিটিটি)}
আমরা ব্যাকপ্রোপাগেশন ব্যাবহার করবো নিউরাল নেটওয়ার্ক ট্রেইন করার জন্য। বুঝার সুবিধার্থে আমরা এই লস ফাংশন নিয়ে বিস্তারিত চিন্তা করবো এখন। আমরা যদি একটা স্টেপ নিই, $t$ থেকে $t+1$ পজিশনে যাই। তাহলে আমাদের লস ফাংশনের $E$ ডেরিভেটিভ ইনপুট $U$ এর সাপেক্ষে দাঁড়ায়। 

\begin{align} % requires amsmath; align* for no eq. number
   \frac {\partial E_{t+1}} {\partial U} = \frac {\partial E_{t+1}} { \partial h_{t+1}} \frac {\partial h_{t+1}} { \partial U} = \frac {\partial E_{t+1}} { \partial h_{t+1}} \frac {\partial h_{t+1}} { \partial h_t} \frac {\partial h_t} { \partial U}\\
\end{align} 

ব্যাকপ্রোপাগেশন থ্রু টাইম হবে - 
\begin{align} % requires amsmath; align* for no eq. number
   \frac {\partial E_{t+1}} {\partial U} = \sum _{k=0} ^{t+1} \frac {\partial E_{t+1}} { \partial h_{t+1}} \frac {\partial h_{t+1}} { \partial h_k} \frac {\partial h_k} { \partial U} \\
\end{align}



\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=\textwidth]{images/lstm_error.pdf} 
   \caption{রিকারেন্ট নেট ইরর}
   \label{fig:rnn_error}
\end{figure} 


এখানে একটা ট্রিক আমরা ইউজ করবো যার সংক্ষিপ্ত নাম বিপিটিটি \cite{58337}। আমরা যদি লম্বা একটা সিকুয়েন্সের উপর ব্যাকপোপাগেশন করতে যাই সেটা মেমোরি এক্সপেন্সিভ এবং কম্পিউটেশনালি স্লো হতে পারে।
তাই আমরা একটা এপ্রোক্সিমেশন ব্যবহার করবো। 


\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=\textwidth]{images/rnn_derivative.pdf} 
   \caption{রিকারেন্ট নেট ব্যাকপ্রোপাগেশন থ্রু টাইম}
   \label{fig:rnn_derivative}
\end{figure} 

\subsubsection{টিচার ফোর্চিং}
\subsubsection{লং টার্ম ডিপেন্ডেন্সি}
আমরা দেখতে পাচ্ছি রিকারেন্ট নেটের টাইম স্টেপ $t$ তার আগের স্টেপ $t-1$ এর উপর নির্ভরশীল। তাহলে আমাদের সিকুয়েন্স যদি ১০০ টা স্টেপ থাকে তবে সর্বশেষ ধাপ ১০০, তার আগের ধাপ 
৯৯ এর উপর নির্ভরশীল। এখানে আমরা রিকারেন্ট ব্যাপারটা খেয়াল করতে পারি। কারণ ৯৯ তম ধাপ তার আগের ধাপ ৯৮ এর উপর নির্ভরশীল। এভাবে করে ৯৮ ধাপ ৯৭ এর উপর নির্ভশীল এবং 
সবার শেষের ০ তে এসে থামবে। লক্ষ্য করা যাক, ১০০ তম ধাপ চেইন ধরে ৯৯ থেকে ০ সবগুলি ধাপের উপরও নির্ভর করছে। কারণ একটাকে কম্পিউট করতে গেলে তার আগেরটার ইনফরমেশন ছাড়া 
কম্পিউটেশন করা সম্ভব না। এই ধরণের ডিপেন্ডেন্সিকে বলা হয় লং টার্ম ডিপেন্ডেন্সি। 

\subsection{এক্সপ্লোডিং ও ভ্যানিশিং গ্র্যাডিয়েন্ট}
রিকারেন্ট নিউরাল নেট ভ্যানিশিং গ্র্যাডেইয়েন্ট প্রব্লেম হয়। সহজ কথায় বললে, মডেল একটা লম্বা সিকুয়েন্স নিয়ে কাজ করতে গেলে আগে যেসব ইনপুট পেয়েছিল সেসব ভুলে যায়।
ভ্যানিশিং গ্র্যাডিয়েন্টের জন্য আমাদেরকে জ্যাকোভিয়ান টার্মটা বুঝতে হবে। 

\subsubsection{জ্যাকোভিয়ান}
\subsubsection{গ্র্যাডিয়েন্ট ক্লিপিং}

\subsection{লং শর্ট টার্ম মেমোরি নেটওয়ার্ক}
আরএনএন এর ভ্যানিশিং গ্র্যাডিয়েন্ট সমস্যা সমাধান করার জন্য লং শর্ট টার্ম মেমোরি নেটোওয়ার্ক ব্যবহার করা হয়। 
\subsubsection{মেমোরি সেল}
যেকোন কম্পিউটেশনাল মেমোরিতে দুই ধরণের অপারেশন সম্ভবঃ ১) রিড ও ২) রাইট। যদি প্রথমবার লিখা হয় সেটাকে বলা হয় ইনসার্ট আর যদি পুরানো লিখা মুছে (ভুলে গিয়ে) নতুন কিছু লিখা হয় সেটাকে বলা হয় আপডেট। 
 
\subsubsection{ইনপুট, ফরগেট ও আউটপুট গেট}
এলএসটীএম মেমোরি সেলে তথ্য আপডেট করার জন্য ৩ টী গেট ব্যবহার করা হয়। এখানে আমরা ধাপে ধাপে ৩ টী গেটের কাজ বুঝার চেষ্টা করবো। 
 
\begin{align} % requires amsmath; align* for no eq. number
   f_t = \sigma \ (W_f \cdot [H_{t-1} , x_t] \ + \ b_f) \\
   i_t = \sigma \ (W_i \cdot [H_{t-1} , x_t] \ + \ b_i) \\
   \tilde C_t = \tanh \ (W_c \cdot [H_{t-1} , x_t] \ + \ b_c) \\
   C_t = f_t * C_{t-1} + i_t * \tilde C_t \\
   o_t = \sigma \ (W_o \cdot [H_{t-1} , x_t] \ + \ b_o) \\ 
   h_t = o_t * \tanh(C_t) \\ 
\end{align}

\subsection{দুটি অত্যাবশ্যকীয় উপাদানঃ নিউরাল এম্বেডিংস + রিকারেন্ট ল্যাঙ্গুয়েজ মডেল}
আমরা যদি সিকুয়েন্সিয়াল ড্যাটা নিয়ে কাজ করতে চাই তাহলে আমাদের এখন দুটি উপাদান লাগবে- ১) ওয়ার্ড ভেক্টরস বা নিউরাল এমবেডীংস ও ২ ) রিকারেন্ট নিউরাল নেটওয়ার্ক। 
আর এই নেটওয়ার্ক ট্রেইন করতে আমরা ব্যবহার করবো বিপিটিটি এলগরিদম। ওয়ার্ড ভেক্টরস আমাদের ফিচার আর রিকারেন্ট নেট আমাদের ডিপ লার্নিং টুল। 
সাধারণত আমরা এলএসটিএম (LSTM) ব্যবহার করার চেষ্টা করবো কারণ ওর লং টার্ম ডিপেন্ডেন্সি নিয়ে কাজ করার সক্ষমতা বেশি। গ্র্যাডিয়েন্ট ইস্যুর কারণে রিকারেন্ট নেট প্র্যাক্টিক্যালি ট্রেইন করা 
কষ্টসাধ্য কাজ। মোটাদাগে, আধুনিক ভাষা প্রযুক্তির সমস্যা সমাধানে এই দুই অস্ত্র থাকলেই কাজ চালিয়ে নেয়া যাবে! 

\textit{কিছু টিপস\#}  সিকুয়েন্সিয়াল ইনপুট আউটপুট বিষয়ক কিছু সূক্ষ্ম বিষয় আছে যা খেয়াল রাখা জরুরি। আমরা যখন কোড লিখবো বা মডেল পার্ফর্মেন্স এনালাইসিস করবো তখন এই বিষয়গুলি কাজে দিবে।
সিকুয়েন্স মডেলিংয়ের কাজ করতে গেলে আমাদের নিচের কয়েকটি বিষয় মাথায় রাখতে হবে। 

\begin{enumerate}[label=(\roman*)]
\item ভ্যারিয়েবল লেংথ সিকুয়েন্স নিয়ে কাজ করতে হতে পারে (যেকোন টেক্সট ইনপুট;  অথচ ছবি চাইলে আমরা ফিক্সড ডাইমেশন রূপান্তর করে নিতে পারি) 
\item শব্দের অর্ডার মনে রাখা (ভাষার বাক্য গঠন বৈশিষ্ট্য) 
\item লং টার্ম ডিপেন্ডেন্সি মনে রাখা (সমৃদ্ধ কন্টেক্সট এর জন্য দরকার) 
\item সিকুয়েন্সগুলির মধ্যে প্যারামিটার (নেটওয়ার্ক ওয়েটস) শেয়ার করা (ট্রেইনিং এফিশিয়েন্সির জন্য; প্রতি স্টেপে আলাদা প্যারামিটার থাকলে প্যারামিটারের সংখ্যা লিনিয়ারলি বেড়ে যাবে!) 

\end{enumerate}

\section{ম্যাশিন ট্রান্সলেশন ও  সিকুয়েন্স টু সিকুয়েন্স মডেল}
\subsection{ম্যাশিন ট্রান্সলেশন টাস্ক}
\subsection{স্ট্যাটিস্টিক্যাল ম্যাশিন ট্রান্সলেশন} 
স্ট্যাটিস্টিক্যাল ম্যাশিন ট্রান্সলেশনের ৩ টি কম্পোনেন্টঃ ট্রানস্লেশন মডেল, ল্যাঙ্গুয়েজ মডেল ও  ডিকোডিং। 

\subsection{সিকুয়েন্স টু সিকুয়েন্স মডেল} 
\subsubsection{এনকোডার ডিকোডার}
আমরা যদি বাংলায় যদি একটা বাক্যকে অনুবাদ করতে চাই তাহলে আমাদের বাংলা বাক্যটি হবে ইনপুট আর ইংরেজি বাক্যটি পাবো আউটপুটে। 
ইনপুট বাক্যকে নিবে এনকোডার। আর এনকোডার এর ইনপুট থেকে আউটপুট বাক্য দিবে ডিকোডার। আমরা এনকোডার ডিকোডার দুটোর জন্য রিকারেন্ট নিউরাল নেট ব্যবহার করতে পারি।
আমাদের একটা আরএনএন (RNN)  লাগবে এনকোডারের জন্য আরেকটা লাগবে আউটপুট ডিকোড করতে। 
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=\textwidth]{images/seq2seq_ilya.pdf} 
   \caption{এনকোডার ডিকোডার মডেল }
   \label{fig:seq2seq}
\end{figure} 


\section{এটেনশন মেকানিজম}
সিকুয়েন্স টু সিকুয়েন্স মডেলে একটি বটলনেক রয়েছে। রিকারেন্ট নেটের সর্বশেষ হিড্ডেন ষ্টেট নিয়ে কেবল কাজ করতে পারবো আমরা। এর মানে আমাদের সকল ইনফর্মেশন একটা জায়গায় ফিক্সড।
এই লাস্ট হিড্ডেন স্টেটকে কন্টেক্সট ভেক্টর বলা হয়ে থাকে। ফিক্সড ভেক্টর ব্যবহার করলে নেটোয়ার্ক লং টার্ম ডিপেন্ডেন্সি নিয়ে কাজ করতে পারে না।
 সেই সমস্যা সমাধানে এটেনশন মেকানিজম ইউজ করা হয়। এমন যদি হয় যে ডিকোডার একটা ফিক্সড কন্টেক্সট না নিয়ে প্রতি টাইম স্টেপে এনকোডারের পুরো সিকুয়েন্স নিয়ে কাজ করতে পারবে?
 পুরো সিকুয়েন্সে যদি এনকোডার এক্সেস থাকে তাহলে এনকোডার যে স্টেটটি বেশি গুরুত্বপূর্ণ সেটিকে ব্যবহার করতে পারবে।  
 এটেনশন ব্যবহার করলে এনকোডারের সবগুলি সিকুয়েন্সের উপর কাজ করা যায় এবং ইনপুটের সবচে উপযোগী অংশ মডেল ব্যবহার করতে পারে। 

\subsection{বাহদান এটেনশন}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.4\textwidth]{images/attention_bahdanau.pdf} 
   \caption{এটেনশন মেকানিজম}
   \label{fig:attn_bahdanau}
\end{figure} 

\subsection{মাল্টি হেড এটেনশন}

\subsection{সেলফ এটেশন}
\subsection{সেক২সেক এপ্লিকেশন}
\subsubsection{নিউরাল ইমেজ ক্যাপশনিং}
\subsection{ট্রান্সফর্মার}
\subsection{সেলফ এটেনশন ও কনভল্যুশনের মধ্যকার সম্পর্ক}

\section{রিকার্সিভ ও কনভল্যুশনাল নিউরাল নেটস ফর ল্যাঙ্গুয়েজ প্রসেসিং}
\subsection{রিকার্সিভ নিউরাল নেটস}
\subsection{কনভল্যুশন}
কনভল্যুশন হলো এক ধরণের ফিচার একস্ট্র্যাকটর। কনভল্যুশন অপারেশনের মাধ্যমে আমরা ইনপুট টেক্সটের লোকাল ফিচারগুলি খুঁজে বের করতে পারি। 
কনভল্যুশন এক ধরণের গাণিতিক অপারেশন যার কাজ হল ইনপুট ম্যাট্রিক্সের উপর স্লাইডিং উইন্ডো ফাংশন এপ্লাই করা।
স্লাইডিং উইন্ডো ফাংশনটীর অনেক নাম, কার্নেল, ফিল্টার বা ফিচার এক্সট্র্যাক্টর। কনভল্যুশন বুঝতে গেলে আমাদের উইন্ডো সাইজের বিষয়টি খেয়াল করতে হবে। 
যেটা হচ্ছে আমরা একটা ফিল্টার নিচ্ছি (ধরা যাক ৩x৩, কার্নেল সাইজ = ৩) এবং পুরো ছবির উপর স্লাইড করছি।
একবার স্লাইড করলে আমরা ইনপুট ম্যাট্রিক্সের উপর একবার স্ট্যাম্প করবো। এখন আমাদের ইনপুটের একটা ব্লক আছে আর ফিল্টার আছে যা ইনপুটের ব্লকের উপর। 
এখানে আমরা এলিমেন্ট ওয়াইজ ম্যাট্রিক্স মাল্টিপ্লিকেশন করে তারপর যোগ করে নিবো।



\subsection{কনভল্যুশন ফর সেনটেন্স মডেলিং}
টেক্সটের জন্য সাধারণত 1D কনভল্যুশন ব্যবহার করা হয় \cite{kalchbrenner-etal-2014-convolutional}।  ধরা যাক, আমাদের কাছে একটি বাক্য রয়েছে।  আমরা উইন্ডো সাইজ নিলাম ২। 1D কার্নেলটি হবে (১x3)!  
এখন আমরা এই দুই উইন্ডো সাইজ নিয়ে টেক্সটের উপর স্লাইড করতে পারি। প্রতিটা স্লাইডিং স্টেপে আমরা একটা নিউমারিক্যাল ভ্যালু পাবে। 
এভাবে পুরো বাক্য স্লাইড করা শেষ হলে আমরা একটা ফিচার পাবো যেটা কো-অকারেন্স ইনফর্মেশন সংগ্রহ করেছে। 
এখন যদি আমরা অনেকগুলি ফিচার ম্যাপ ব্যবহার করে তাহলে আমরা নানান রকম ফিচার এক্সট্র্যাক্ট করতে পারবো। 

মজার বিষয়,  আমরা চাইলে যেকোন উইন্ডো সাইজ ব্যবহার করতে পারি। সাধারণত উইন্ডো সাইজ হতে পারে ১,২,৩। 
এখানে লক্ষ্য করলে বুঝা যাবে আমরা যখন স্লাইড করছি তখন কো-অকারেন্স কম্পিউটেশন হচ্ছে। মানে আমরা সেন্টার পয়েন্টের আশেপাশে কোন শব্দগুলি আছে সেগুলিকে ফিল্টার ম্যাপের আওতাও বিবেচনা করছি।
ধরা যাক, ৩x৩ ফিল্টার সাইজ, সেন্টার ওয়ার্ড হলো মাঝখানে শব্দটি আর ডান বাঁয়ে দুটি শব্দ আমাদের সেন্টার ওয়ার্ডের প্রতিবেশী শব্দ। আমরা যদি এক ধাপ স্লাইড করি তাহলে সেন্টার ওয়ার্ডটি 
তখন পরের ধাপে হয়ে যাবে প্রতিবেশী শব্দ। কনভল্যুশন কো-অকারেন্সের উপর নির্ভর করে আমাদেরকে ভালো স্প্যাশিয়াল ফিচার দিবে। মানে কোন শব্দ কার আশেপাশে বসতে পারে এই ইনফর্মেশন 
মডেল শিখবে ইনপুট থেকে। উদাহরণ বাক্য, আমার মন ভালো। মন শব্দের পর সাধারণত "ভালো",  "খারাপ" এই শব্দগুলি নিয়মিত বসে। আমরা যদি অনেক ইনপুট নিয়ে কাজ করি মডেল এই কো-অকারেন্স 
বৈশিষ্ট্য শিখে নিতে পারবে। যদি এটা মডেল বুঝতে পারে তাহলে সে বাক্যের গঠন সম্পর্কে এক ধরণের লার্নিং পাচ্ছে যা ফিচার হিশেবে গুরুত্বপূর্ণ।

\subsubsection{n-গ্রাম ও 1D কনভল্যুশন}
মন দিয়ে লক্ষ করলে বুঝা যাবে, কনভল্যুশন অপারেশনের সাথে n-gram মডেলের সাদৃশ্য আছে। 
লক্ষ্য করি, ফিল্টার সাইজ ২ হলে এটা একটা ২ গ্রাম মডেল কেননা মডেল ঠিক বাইগ্রামের মতন করে ফিচার তইরি করবে। 
আমরা কনভনেটকে n-গ্রাম ফিচার এক্সট্যাক্টর হিশেবে চিন্তা করতে পারি। এখন আমাদের পরের ধাপে কাজ হবে ফিচারগুলি জোড়া দিয়ে নেটওয়ার্ক ট্রেইন করা।

কনভল্যুশনকে ন্যাচারাল ল্যাংগুয়েজ প্রোসেসিংয়ে phrase বেইজড মডেল হিশেবে ব্যবহার করা হয়। রিকারেন্ট নেটে সিকুয়েন্সিয়াল ডিপেন্ডেন্সি থাকে। 
রিকারেন্ট নেট prefix নিয়ে কাজ করে আর কনভল্যুশন নেটওয়ার্ক n-gram phrase নিয়ে কাজ করে।
একটা বাক্য রিকারেন্ট নেটওয়ার্ক প্রসেস করবে সিকুয়েন্সিয়াল ডিপেন্ডেন্সি তইরি করে আর কনভল্যুশন নেটওয়ার্ক অনেকগুলি phrase থেকে ফিচার নিবে। 
কনভল্যুশন নেটোয়ার্কের ইনপুট দিবো ওয়ার্ড ভেক্টরস। আমরা অনেকগুলি ফিল্টার ব্যবহার করতে পারি। সাধারণত ফিল্টার সাইজ ২,৩,৪,৫ হয়ে থাকে। 
লক্ষ্য করি, ফিল্টার সাইজ ২ হলে এটা একটা ২ গ্রাম মডেল কেননা মডেল ঠিক বাইগ্রামের মতন করে ফিচার তইরি করবে। 
আমরা কনভনেটকে n-গ্রাম ফিচার এক্সট্যাক্টর হিশেবে চিন্তা করতে পারি। 

\subsubsection{পুলিং ও ডিপ স্ট্যাকিং}
এখন প্রশ্ন হচ্ছে কনভল্যুশন লেয়ারে অনেকগুলি ফিচার আমরা পাবো। সবগুলি ফিচার কি আমাদের কাজে লাগবে? 
উত্তর হচ্ছে, না! এইজন্য পরের লেয়ারে আমরা একটি পুলিং অপেরেশন করবো। পুলিং অপারেশনের কাজ হলো সবচে ভালো ফিচার ভেক্টরগুলি খুঁজে বের করা। 
তারপর আমরা একটি সফটম্যাক্স লেয়ার ব্যবহার করে ক্লাসিফিকেশন কাজ করতে পারি। আমরা চাইলে কনভল্যুশন + পুলিং একটা পর একটা স্ট্যাক করতে পারি। 
ডিপ নিউরাল নেটওয়ার্ক স্ট্যাকিং ভালো ক্লাসিফিকেশন পার্ফমেন্স দেয়।

\subsection{কনভল্যুশন ফর সেনটেন্স ক্লাসিফিকেশন}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.8\textwidth]{images/cnn_full_zhangetal_2015.png} 
   \caption{সেন্টেন্স ক্লাসিফিকেশনের জন্য কনভল্যুশন নিউরাল নেটওয়ার্ক আর্কিটেকচার ইলাস্ট্রেশন}
   \label{fig:cnn_zhangetal} 
\end{figure} 

\subsection{কনভনেটস ট্রেইনিংঃ  কিছু প্র্যাক্টিক্যাল ট্রিকস}

\subsection{রিকারেন্ট ও কনভল্যুশনাল নিউরাল নেটওয়ার্কের তুলনামূলক আলোচনা}

\section{প্রিটেইন্ড ল্যাঙ্গুয়েজ মডেলস}
\subsection{কনটেক্সুলাইজড রিপ্রেজেন্টেশন্স}
যদি আমরা ওয়ার্ড২ভেকে ওয়ার্ড অর্ডার রাখতে পারি তাহলে কি রিপ্রেজেন্টেশন আরো ভালো হবে না? আমরা অনেক ভালো সিমান্টিকস এবং সিনট্যাক্স 
ইনফরমেশন ম্যাশিনকে দিতে পারবো। 

\subsection{প্রি-টেইনিং}

\subsection{বার্ট (BERT)}

\subsection{জিপিটি ৩ (GPT 3)}


 \part{পার্ট ৪} 

\section{GLUE বেঞ্চমার্ক টাস্কস}
গ্লু বেঞ্চমার্ক \cite{wang-etal-2018-glue}

\subsection{সেন্টেন্স ক্লাসিফিকেশন}
\subsection{ন্যাচারাল ল্যাঙ্গুয়েজ  ইনফারেন্স}
\subsection{কোশ্চেন আনসারিং}


\section{ট্রান্সফার লার্নিং ফর ল্যাঙ্গুয়েজ প্রসেসিং}
এনএলপি তে কিছু রিয়েল লাইফ চ্যালেঞ্জ রয়েছে। সাধারণ ডিপ লার্নিং সিস্টেম ট্রেইন করতে গেলে অনেক ড্যাটা লাগে। ধরা যাক, কুশ্ছেন আনসারিং সিস্টেম বা ম্যাশিন ট্রান্সলেশন। আমাদের কয়েক হাজার ট্রেইনিং স্যাম্পল লাগবে একটা ভালো সিস্টেম ট্রেইন করতে।  অনেক ডিপ লার্নিং সমস্যা সমাধানে যথেষ্ট ড্যাটা পাওয়া যায় না। ধরা যাক, ৫০০০০ এর বেশি ট্রেইনিং স্যাম্পল লাগছে ট্রেইনিং করানোর জন্য। আরেকটা বিষয় হচ্ছে সোর্স এবং টার্গেট ডিস্ট্রিবিউশন সমান হতে হবে। এই সমস্যার সহজ সমাধান হচ্ছে টার্গেট ড্যাটা নিয়ে আরেকটি মডেল ট্রেইন করানো। এবং টার্গেট ডোমেইনে ল্যাবেলড ডাটা পাওয়া মুশকিল। আমরা দেখা গেলো একটা ড্যাটাসেট আমরা নর্থ এমেরিক্যান কনটেক্সটে বানালাম। সেটা এশিয়ান এই সমস্যাগুলি সমাধানে ট্রান্সফার লার্নিং ব্যবহার করা হয়। কিন্তু অন্য কোন সমস্যা সমাধান করতে গিয়ে আমরা যা শিখেছি সেটা আমরা কাজে লাগাতে পারি। এটাকে বলা হয় ট্রান্সফার লার্নিং। শব্দ দুটি দেখেই বুঝা যাচ্ছে লার্নিং কে আরেকটা কাজে ট্রান্সফার করা হবে। 

প্রিটেইন্ড ল্যাঙ্গুয়েজ মডেলের জন্য ট্রান্সফার লার্নিং খুবই নিয়মিত এখন। ট্রান্সফার লার্নিং অনেকগুলি টপিক আছে। আমাদের কিছু এজাম্পশন দিয়ে কাজ আগাতে হবে। প্রথমঃ আমার টার্গেটে ল্যাবেলড ড্যাটা আছে (ইনডাক্টিভ ট্রান্সফার লার্নিং) , ল্যাবেলড ড্যাটা কেবল সোর্সেই আছে (ট্রান্সডাক্টিভ ট্রান্সফার লার্নিং)। আরেকটা হচ্ছে সোর্স এবং টার্গেট ল্যাবেলড ড্যাটা নেই (আনসুপারভাইজড ট্রান্সফার লার্নিং)। ডোমেইন এডাপটেশন হল ট্রান্সডাক্টিভ ট্রান্সফার লার্নিং এর একটি উদাহরণ। ডোমেইন এডাপটেশনের একটা উদাহরণ হতে পারে আপনার NER টাস্ক এর জন্য ইংরেজি ট্রেইনিং ড্যাটা আছে কিন্তু বাংলায়। দুটি আলাদা ভাষা ডোমেইন আলাদা করেছে। এখানে টাস্ক সেইম। আমাদের কাজ হচ্ছে ইংরেজি ডোমেইন থেকে বাংলায় নলেজ ট্রান্সফার করা।  কোভ্যারিয়েট শিফট হলো অপটিমাল প্রেডিক্টর ফিক্সড, কিন্তু ফিচার স্পেস ডিফারেন্ট যেখানে সোর্স আর টার্গেট টাস্ক একই। পার্টস অব স্পিচ ট্যাগিং এবং নেইম এন্টিটি দুটি টাস্ক অনেকখানি সিমিলার। এনইআর ক্ষেত্রে লো লেভেল সিনট্যাক্টিক ইনফরমেশন দরকার হয়। পার্টস অব স্পিচ ট্যাগিংয়েও সিনট্যাক্স ফিচার গুরুত্ব বহন করে। ব্যাপারটা এমন যে আপনি যদি POS ট্যাগ জানেন কোন একটা বাক্যের তাহলে সেগুলি ব্যবহার করে NER করতে প্রচুর হিউরেস্টিকস পাওয়া যায়। POS ট্যাগের তথ্য NER টাস্কটি সহজ করে দেয়। আমাদের এই দুটি টাস্কের জন্য ল্যাবেলড ড্যাটা আছে। আমরা যদি এই দুটি টাস্ক একসাথে ট্রেইন করাই এবং প্যারামিটার শেয়ার করে দিই তাহলে POS যা শিখছে এবং NER যা শিখছে দুটি টাস্ক একটা আরেকটাকে শেয়ার করতে পারে। এটাকে বলা হয় ইন্ডাক্টিভ ট্রান্সফার লার্নিং। ইন্ডাক্টিভ লার্নিং মানে হলো ইমপ্লিচিট লার্নিং। মানে আপনি যখন একের অধিক টাস্ক একসাথে শিখছেন  তখন একে অপরকে অটোমেটিক্যালি সাহায্য করবে, কোন এক্সপ্লিসিট অবজেক্টিভ ছাড়াই। এটা লার্নিং প্রসিডিউর নিজে নিজেই এই প্রোপার্টি এমার্জ করে। ইনডাক্টিভ ট্রান্সফার লার্নিংয়ের চমৎকার উদাহরণ হলো মাল্টি টাস্ক লার্নিং। কিন্তু ট্রান্সডাক্টিভ ট্রান্সফার লার্নিংয়ে  আমরা টার্গেট ডোমেইনে এক্সপ্লিসিটলি নলেজ ট্রান্সফার নিয়ে অবজেক্টিভ যেমন লস ফাংশন, আর্কিটেকচার ডিজাইন করি। আনসুপারভাইজড লার্নিং ক্ষেত্রে আমরা লেলেবেলড ড্যাটা থেকে শেখার সুযোগ নাই। আমরা বলেছি আগে প্রবাবিলিস্টিক ম্যাশিন লার্নিংযের উদ্দেশ্য হলো ড্যাটা জেনারেটির প্রসেস শেখা বা এপ্রোক্সিমেট করা। ধরা যাক, আপনার কাছে d-ডাইমেনশনাল ড্যাটা আছে। এবং এটা  প্লট করলে আপনি একটা হিস্টোগ্রাম পাবেন। ডেনসিটি এস্টিমেশনের কাজ হলো এই হিস্টোগ্রাম প্লটটা এপ্রোক্সিমেট করা। কারণ মূলত আপনি ড্যাটা জেনারেটিং ডিস্ট্রিবিউশন জানেন না। আপনার কাছে আছে কিছু স্যাম্পল ড্যাটা সেগুলি থেকে আপনি স্ট্রাকচারটা এস্টিমেট করে নিচ্ছেন ম্যাশিন লার্নিং মডেল দিয়ে। রিয়েল ওয়ার্ল্ড ড্যাটা ডিস্ট্রিবিউশন সাধারণত হাইডাইমেনশনাল এবং কমপ্লেক্স। এমন কমপ্লেক্স ডিস্ট্রিবিউশন জানা যায় না অধিকাংশ ক্ষেত্রে। আমরা জানি কয়েন ফ্লিপিং হলো বার্নুলি ডিস্ট্রিবিউশন। ডিস্ট্রিবিউশন জানা থাকলে ওখান থেকে আমরা ড্যাটা স্যাম্পল করতে পারবো। বার্নুলি $\gamma$ ভ্যালু জানলে, আমরা কয়েন ফ্লিপ হেড টেইল জেনারেট করতে পারবো। আমাদের ম্যাশিন লার্নিং টাস্ক হলো এই ডিস্ট্রিবিউশন এপ্রক্সিমেট করা। যখন আমরা ডিস্ট্রিবিউশন জানবো, তখন আমরা একইসাথে ড্যাটা জেনারেটিং প্রসেস ইমিটেট করতে পারবো। আমরা যে দুটি ট্রান্সফার লার্নিং সমস্যা নিয়মিত ম্যাশিন লার্নিং কম্যুনিটিতে পাই একটি হলো ডোমেইন এডাপটেশন আরেকটি মাল্টিটাস্ক লার্নিং। 

ট্রান্সডাক্টিভ ট্রান্সফারের কিছু বৈশিষ্ঠ্যঃ i) টার্গেটে ডমাইনে কোন ল্যাবেলড ড্যাটা নেই ii) এনএলপি ট্রান্সফার লার্নিং রিসার্চ মোস্টলি এই টপিকে হয় এবং   iii) উদাহরণ ডোমেইন এডাপটেশন। ইনডাক্টিভ ট্রান্সফার লার্নিংয়ের বৈশিষ্ঠ্যঃ i) টার্গেটে ল্যাবেলড ড্যাটা আছে ii) লক্ষ্য হলো অন্য টাস্কের সাহায্য নিয়ে টার্গেট টাস্কের পারফর্মেন্স বাড়ানো। iii) একসাথে বিভিন্ন টাস্ক শিখানো (মাল্টিটাস্ক লার্নিং) , প্রি-ট্রেইনিং (ওয়ার্ড এম্বেডিংস)। যেমন সিমান্টিক সিমিলারিটি শেখা ওয়ার্ড এম্বেডিংসের অবজেক্টিভ ফাংশনে নেই। লার্নিং থেকে এই প্রোপার্টি এমার্জ করে। আরেকটা উদাহরণ হতে পারে BERT ট্রেইনিং। সিন্ট্যাক্টিক ফিচার শেখা প্রিটেইনিংয়ের অবজেক্টিভ না। প্রিটেইনিং অবজেক্টিভ হলো মাস্কড ল্যাংগুয়েজ মডেলিং। আমরা যখন BERT ট্রেইন করছি তখন ব্যাগ অব ওয়ার্ডস, সিনট্যাক্স এগুলি শেখাই না তবু প্রথম দিককার লেয়ারগুলি এগুলি শিখে নেয়। এটাই হলো ইন্ডাক্টিভ ট্রান্সফার। এগুলি সে শিখতে পারে ল্যাংগুয়েজ মডেলিং শিখতে গিয়ে।



\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.6\textwidth]{images/transfer_learning_scenario.png} 
   \caption{ট্রান্সফার লার্নিং}
   \label{fig:ruder_transfer_learning}
\end{figure} 

\subsection{সিকুয়েন্সিয়াল ট্রান্সফার}

\subsubsection{প্রি- ট্রেইনিং}

ওয়ার্ড এম্বেডিংসঃ প্রি টেইন্ড ওয়ার্ড এম্বেডিংস অধিকাংশ ডিপ লার্নিং মডেলের প্রথম লেয়ারে থাকে। প্রি টেইন্ড এম্বেডিং কিছু সমস্যা আছে। এটা শ্যালো এপ্রোচ। ওয়ার্ড এম্বেডিংস কনটেক্সট সেনসিটিভ না। এবং এগুলি সেন্স ধরতে পারে না। সাধারণত প্রথম লেয়ার প্রিটেইন্ড করা থাকে, বাকি লেয়ারগুলি শুরু থেকে ট্রেইন করতে হয়। এনএলপি প্রিটেইনিং যে নতুন প্যারাডাইম শিফট সেটা এসছে ইমেজনেট প্রিট্রেইনিং কম্পিউটার ভিশন থেকে। যেমন VggNet, ResNet। আমাদের কাছে যদি একটা ইমেজ থাকে তাহলে আমরা সেটা VGGnet এ ইনপুট হিশেবে দিলে সে একটা আউটপুট ফিচার দিবে। সেই ফিচার দিবে আমরা সহজেই ক্লাসিফাইয়ার ট্রেইন করাতে পারবো। ভিশনে এখন প্রিটেইন্ড মডেল ইমেজনেট একটা স্টান্ডার্ড। প্রথম লেয়ারে এজ, তারপর টেকচার, প্যাটার্ন, তারপর পার্টস এবং শেষ লেয়ারে অবজেক্ট থাকে। এখন আমরা যেই টাস্কটি সলভ করতে চাই হয়ত সেটই অবজেক্ট রিকগনিশন নয়। কিন্তু টেকচার সেই টাস্কের জন্য গুরুতপূর্ণ। সেক্ষেত্রে আমরা শুরু থেকে টেকচার লেয়ার কে প্রিটেইন্ড লেয়ার হিশেবে নিতে পারি এবং পরে কিছু লেয়ার জোড়া দিয়ে একটা নতুন মডেল বানাতে পারবো। আধুনিক ম্যাশিন লার্নিংয়ে প্রিটেইনিং ট্রান্সফার লারনিংযের একটা key কনসেপ্ট হিশেবে কাজ করে। এখন প্রশ্ন হচ্ছে ল্যাঙ্গুজে প্রসেসিংযে ইমেজনেট টাস্কের মতন টাস্ক কি হতে পারে? ইমেজনেট থেকে আমরা কিছু আইডিয়া নিতে পারি। এনএলপি ইমেজনেট ডাটাসেটটি যথেষ্ট বড় হতে হবে যেমন মিলিয়নস অব ট্রেইনিং স্যাম্পল। এটা সাধারণ প্রব্লেম স্পেসকে রিপ্রেজেন্ট করতে হবে। এই ধরণের কিছু টাস্ক যদি চিন্তা করি তাহলে পাবোঃ i), রিডিং কম্প্রিহেনশন (SQuad ড্যাটাসেট, 100k Q-A জোড়া)  ii) ন্যাচারাল ল্যাংগুয়েজ ইনফারেন্স (SNLI করপাস, 570k বাক্য জোড়া) iii) ম্যাশিন ট্রান্সলেশন (WMT 2014, 40M ফ্রেঞ্চ - ইংরেজি বাক্য জোড়া)  iv) ল্যাংগুয়েজ মডেলিং (অফুরন্ত ড্যাটা, এখন বেঞ্চমার্ক ড্যাটাসেট সাইজঃ 1B শব্দ)। কনসেপ্টটি হচ্ছে আপনার কাছে যদি বড় কোশ্চেন আনসারিং ড্যাটাসেট থেকে ট্রেইন্ড করা মডেল থাকে তাহলে সেই মডেলকে আপনি অন্য টাস্ক সলভ করার জন্য ব্যবহার করতে পারেন। কারণ ড্যাটাসেট সাইজ অনেক বড় এবং ট্রেইনিং থেকে যে রিপ্রেজেন্টেশন লেয়ার পাওয়া যায় সেগুলি অন্য টাস্ককে সাহায্য করে। উপরের প্রথম ৩ টি টাস্ক ল্যাবেলড ড্যাটা দরকার পড়ে এবং ল্যাবেলড ড্যাটা সবসময় সীমিত। শেষ টাস্কে কোন ল্যাবেলড ড্যাটা লাগে না, যেমন 1B শব্দ আমরা ব্যবহার করতে পারি তাই এই ধরণের লার্নিং সিস্টেম ভালো প্রিট্রেইনিং ফলাফল দেয়। 

ল্যাংগুয়েজ মডেলিং ভাষার বেশ কয়েকটি ক্যাপচার আস্পেক্ট করে। যেমন লংটার্ম ডিপেন্ডেন্সি, হায়ারার্কিক্যাল সম্পর্ক, সেন্তিমেন্ট ইত্যাদি। যেমন আমরা যে অবজেক্টিভ ফাংশন ব্যবহার করে তার মধ্যে আগের কন্টেক্সট এর উপর নির্ভর করে পরের বাক্য প্রেডিক্ট করতে তাই। এটা লং টার্ম ডিপেন্ডেন্সি ছাড়া সম্ভব নয়। সেন্টিমেন্ট আমরা ওয়ার্ড এম্বেডিংস দেখতে পাই খুব সহজেই। কাছাকাছি অর্থ বহন করে এমন শব্দ একই ক্লাস্টারে থাকে। ট্রেইনিং ড্যাটা তো আনলিমিটেড থাকছেই।  


\subsubsection{ULMFit} 
এই সেকশনে আমরা সিকুয়েন্সিয়াল ট্রান্সফার লার্নিং নিয়ে একটা ম্যানুয়াল আলোচনা করতে চাই। পেপারটির শিরোনাম হল Universal Language Model Fine-Tuning (ULMFiT)। আমাদের যে এপ্রোচ সেটি ইন্ডাক্টিভ ট্রান্সফার লার্নিং। 



\subsection{মাল্টি-টাস্ক ট্রান্সফার লার্নিং}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.6\textwidth]{images/mtl_images_ruder.png} 
   \caption{মাল্টি টাস্ক লার্নিং}
   \label{fig:ruder_mtl}
\end{figure} 
\subsection{লাইট ওয়েট ফাইন-টিউনিং}

\section{এথিকস ও ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং}
\subsection{জেন্ডার বায়াস ও ওয়ার্ড  এম্বেডিং}
\subsection{ডিবায়াসিং}
\subsection{স্টকাস্টিক প্যারটস}

\section{ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং রিসার্চ}
\subsection{থিম ১ঃ ল্যাঙ্গুয়েজ মডেলিং}
\subsection{থিম ২ঃ কমনসেন্স রিজনিং}
\subsection{থিম ২ঃ মডেল এনালাইসিস ও এক্সপ্লেনেশন}
\subsection{থিম ৩ঃ  মাল্টিলিঙ্গুয়াল ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং} 
\subsection{থিম ৪ঃ কন্টিনিউয়াল লার্নিং}


\bibliographystyle{unsrt}
\bibliography{ref}
\end{document}