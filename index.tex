%!TEX program = xelatex

\documentclass{article}[book]
\usepackage{geometry}

 \geometry{
 letterpaper,
 left=1in,
 right=1in,
 top=20mm,
bottom=20mm,
paperheight=11in,
paperwidth=8.5in}

\usepackage{enumitem}

\usepackage{bookmark}  
\usepackage{svglatex}
\usepackage{amsmath}

\usepackage{datetime}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{xcolor}

\usepackage{tcolorbox}

\usepackage{titlesec,titletoc}

%used to import graphics%
\usepackage{graphicx}

%keep only the page number%
%\pagestyle{plain}

% adding fontspec pkg and kalpurush font
\usepackage{fontspec}
\usepackage{polyglossia}
\setdefaultlanguage{bengali}
\setotherlanguages{english}
\newfontfamily{\bengalifont}[Script=Bengali]{Kalpurush}


\title{ডিপ লার্নিং ও ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং}
\author{রবিউল আউয়াল}
\date{১৭ জুলাই, ২০২১}



\begin{document}
\maketitle

\tableofcontents


\section{পরিচিতি }
বাংলায় ন্যাচারাল ল্যাঙ্গুয়েজ  বই লিখার চেষ্টা করছি। 
২০২০ সালের মাঝামাঝি সময়ে আমরা একটা রিডিং গ্রুপ শুরু করি। রিডিং কোর্সের জন্য এই বইটি সহায়ক হিশেবে কাজ শুরু করি।
আমাদের লেকচারগুলি থেকে গুরুত্বপুর্ণ কম্পোনেনট নিয়ে বইটি সাজানো হয়েছে। এতে প্রায় ১৬ টি অধ্যায় রয়েছে। মোট ৪ খণ্ডে বইটি ভাগ করা হয়েছে।   
এই বইকে ডিপ লার্নিং ফর ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং হিসেবে ব্যবহার করা যেতে পারে। 
ন্যাচালার ল্যাঙ্গুয়েজ প্রসেসিং, বাংলায় বলছি ভাষাপ্রযুক্তি। যে টপিকগুলি নিয়ে আলোচনা করা হয়েছে সেগুলির মধ্যে থাকছে - প্রশ্ন উত্তর, ম্যাশিন ট্রান্সলেশন (অনুবাদ) , চ্যাটবট, সিকুয়েন্স ল্যাবেলিং এগুলি হলো জনপ্রিয় সমস্যা। আমার নিজের গবেষণায় খুব একটা উন্নতির আশা দেখছি না। যেহেতু গত দুই বছর ধরে ভাষা প্রযুক্তি নিয়ে পড়ছি। আমি ভাবলাম এই ফিল্ডে যেসকল চমৎকার কাজ হয়েছে, সেগুলি নিয়ে 
আলাপ করতে পারলে কিছুটা মনোবাসনা পূর্ণ হবে। একটা কথা প্রচলিত আছে যেসব লোকেরা নাকি কবিতা লিখতে গিয়ে ব্যর্থ হইছেন তারাই নাকি পরবর্তীতে বাংলা বিভাগের অধ্যক্ষ হন এবং কবিদের ঘাড় মটকানোর 
কাজটি করে বেড়ান পেশাদারিত্বের সাথে। যদিও আমার ইচ্ছে সেরকম কিছু নয়। যাহোক মুগ্ধতার বয়ান আপনাদের সামনে উপস্থাপন করা যাক তবে।

 \part{পার্ট ১} 
\section{ম্যাশিন লার্নিং}
\subsection{সুপারভাইজড লার্নিং সেটআপ}
\subsubsection{ইনপুট, আউটপুট}
\subsubsection{মডেল}
\subsubsection{কস্ট ফাংশন}

\section{ম্যাশিন লার্নিং এলগরিদমস}

\subsection{লজিস্টিক রিগ্রেশন}
\subsection{লিনিয়ার রিগ্রেশন}
\subsection{রেগুলারাইজেশন}

\section{ডীপ লার্নিং}
\subsection{নিউরাল নেটস}
\subsection{ট্রেইনিং নিউরাল নেটস}
\subsection{ব্যাকপ্রোপাগেশন}
\subsection{অপটিমাইজেশন}
\subsection{প্র্যাক্টিক্যাল টিপস ও পাইটর্চ}


 \part{পার্ট ২} 
 
\section{টার্মিনোলজিঃ  হাতি ঘোড়া বাঘ}
\subsection{ল্যাঙ্গুয়েজ}
\subsection{সিকুয়েন্স} 
\subsection{সিমান্টিকস} 
\subsection{সিনট্যাক্স}
\subsection{কনটেক্সট}
\subsection{ফাংশন এপ্রোক্সিমেশন}
\subsection{ল্যাংগুয়েজ এজ ফাংশন এপ্রোক্সিমেশন} 
 
 
\section{ডিস্ট্রিবিউটেড সিমান্টিকস}
\subsection{ডিস্ট্রিবিউটেড ওয়ার্ড রিপ্রেজেন্টেশন}
আমাদের কাছে একটি বড় করপাস আছে। একটা বাক্যের টার্গেট এবং কনটেক্সট এই দুটো টার্ম আমাদের প্রথমে বুঝতে হবে। 
বাক্যগুলিতে  ম্যাশিনের ইনপুট হিশেবে ব্যবহার করার জন্য আমাদের নিউমারিক্যাল ভ্যালুতে রূপান্তর করতে হবে। 
ওয়ান-হট-ভেক্টর। যেখানে একটা ভেক্টরের সবগুলি আইটেমের মান শূন্য শুধু একটা আইটেমের মান ১। ওয়ার্ড টু ভেক্টর হলো একটা টেবিল। 
একে ওয়ার্ড এম্বেডিং বলা হয়। যেকোন বাক্যের এম্বেডিং নিউরাল নেটওয়ার্কের জন্য ইনপুট বা ফিচার হিশেবে কাজ করে।
ওয়ার্ড ভেক্টর অনেক সিমান্টিক তথ্য সংগ্রহ করে রাখতে পারে যা নিউরাল নেটওয়ার্ক ট্রেইনিংয়ে সাহায্য করে।
কন্টিনিউয়াস ব্যাগ অফ ওয়ার্ডস মডেল (সিভোও)। আরেকটি মডেল হচ্ছে স্কিপ গ্রাম মডেল। আমরা যেকোন একটি মডেল ব্যবহার করতে পারি। 
 
 \subsection{নিউরাল ওয়ার্ড এম্বেডিংস}
\subsubsection{ওয়ার্ড ভেকটরস (word2vec) }
ওয়ার্ড টু ভেক হলো আমরা একটা টেবিল চিন্তা করতে পারি। সেই টেবিল থেকে ওয়ার্ড যেখানে ম্যাচ করছে সেই ভ্যালুটা রিটার্ন করবে। 

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.8\textwidth]{images/cbow_skipgram.pdf} 
   \caption{ওয়ার্ড টু ভেক মডেল}
   \label{fig:cbowskipgram}
\end{figure} 


\subsubsection{স্কিপ গ্রাম (Skip-gram)}
\subsubsection{কন্টিনিউয়াস ব্যাগ অব ওয়ার্ডস (CBOW)}
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.8\textwidth]{images/cbow_model.pdf} 
   \caption{কন্টিনিওয়াস ব্যাগ অব ওয়ার্ডস মডেল}
   \label{fig:cbow}
\end{figure} 

\subsubsection{নেগ্যাটিভ স্যামপ্লিং}
নেগেটিভ স্যামপ্লিং হলো পজিটিভ উদাহরণের সাথে নেগেটিভ উদাহরণ ব্যবহার করা। একে কন্ট্রাসটিভ লার্নিং ও বলে। 
\subsection{নিউরাল ওয়ার্ড এম্বেডিংস এভালুয়েশন }

\subsubsection{আউট অব ভোকাবুলারি ওয়ার্ডস ও ক্যারেক্টার কমপোজিশনাল মডেল}


\section{সিকুয়েন্স ল্যাবেলিং }
\subsection{সিকুয়েন্স ল্যাবেলিং ও ক্লাসিফিকেশন}
\subsection{সিকুয়েন্স ল্যাবেলিং এপ্লিকেশন}
\subsubsection{পার্টস অফ স্পিচ ট্যাগিং}
\subsubsection{নেইম এন্টিটি রিকগনিশন}
ল্যাংগুয়েজ প্রোসেসিংয়ে আমরা যে নেটওয়ার্কগুলি ব্যবহার করে থাকি সেগুলি যেকোন একটা ব্যবহার করতে পারি। আমরা ওয়ার্ড এমবেডিং ইউজ করতে পারি, এলএসটিএম ব্যবহার করতে পারি বা কনভনেটস ব্যবহার করতে পারি। সবগুলি নেটোয়ার্কের শেষে আমরা একটা লিনিয়ার লেয়ার ব্যবহার করবো। লিনিয়ার লেয়ারের পরে একটা সফটম্যাক্স প্রেডিকশন হবে। তখন সে আমাকে যতগুলি ট্যাগ আছে সেগুলির আউটপুট দিবে। যদি ৯ টা ক্লাস থাকে সেগুলির থেকে যেকোন আউটপুট হবে। ল্যাংগুয়েজ ইনপুট সিকুয়েন্স চলক দৈর্ঘ্যের হয়ে থাকে। কোন বাক্যের দৈর্ঘ্য হতে পারে ১০, আরেকটি ১২, আরেকটি ৩০। আমরা বাক্যগুলিকে রিশেইপ করে একটি ফিক্সড লেংথে নিয়ে আসবো। রিকারেন্ট নেটওয়ার্ক একটা ফিক্সড লেংথ ইনপুট চায়। আমার বাক্যের দৈর্ঘ্য যদি ১০ হয়, তাহলে বাকি পজিশনগুলিতে আমি ০ দিয়ে দিবো। ও থাকা মানে এই জায়গাগুলিতে অতিরিক্ত প্যাডিং করা হয়েছে। প্যাডিং মানে বাকি ২০ টা টোকেন অতিরিক্ত dummy ও দিয়ে ভরে ফেলা। এখন সিকুয়েন্স প্রেডিকশনের কাজে এই প্যাডিং কাজে দিবে। আমি যে টোকেনগুলি পাচ্ছি, তার যেগুলির মান শূন্য হবে সেগুলির জন্য আমি লস হিশেব করবো না। অই স্থানে মডেলের আউটপুটের জন্য কোন লস কম্পিউট করা হবে না। কারণ যেহেতু ভ্যালু ও তাই এটা আমরা প্যাড করে এনেছি। আপনি যদি মাস্কিং ব্যবহার করেন তাহলে ১০ টা সিকুয়েন্সের জন্য ১০ টা আউটপুত পাবেন এবং লস ১০ টা টোকেনের জন্য আসবে। নিজের কোডিং করতে গেলে এই বিষয়টি মাথায় রাখতে হবে। আপনার নেটওয়ার্ক ফিক্সড লেংথ কিন্তু ইনপুট ভ্যারিয়েবল লেংথ। নেটওয়ার্কগুলি নিজেরা স্মার্ট না তাই এটা এক্সলিসিটলি হ্যান্ডেল করতে হবে যে এখানে আউটপুট হবে না। প্রশ্নঃ যদি ভ্যালু ০ শূন্য হয় তাহলে যদি বাদ দিয়ে দিই, তাহলে মূল সিকুয়েন্সে ০ থাকে? এক্ষেত্রে সচেতন থাকতে হবে যে এম্বেডিং ০ থাকবে না কোন টোকেন এনকোড করার জন্য। যদি এমন হয় এম্বেডিং ০ আছে, তাহলে আমরা -১০০০ দিয়ে মাস্কিং করে নিতে পারি। Glove এমেবডিং ০ নেই, তাই আমরা ০ ব্যবহার করতে পারি। এবার তাহলে রিকারেন্ট নেটওয়ার্ক দিয়ে নেইমড এন্টিটি রিকগনিশন নিয়ে আলাপ করা যাক। 


\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includesvg[width=0.8\textwidth]{images/rnn_seq.svg} 
   \caption{রিকারেন্ট নেটওয়ার্ক সিকুয়েন্স ট্যাগিং}
   \label{fig:cbow}
\end{figure} 


\section{লিঙ্গুইস্টিক স্ট্রাকচার}
\subsection{ডিপেন্ডেন্সি পার্সার}
\subsection{ইউনিভার্সাল ডিপেন্ডেন্সিস}

\section{রেফারেন্স রেজ্যুলুশন}

 \part{পার্ট ৩} 
\section{রিকারেন্ট নিউরাল নেটওয়ার্কস ও ল্যাঙ্গুয়েজ মডেল}
\subsection{n গ্রাম ল্যাঙ্গুয়েজ মডেল}
\subsubsection{ল্যাঙ্গুয়েজ মডেল কি}
ল্যাঙ্গুয়েজ মডেল হচ্ছে পরবর্তী স্ট্রিং প্রেডিকশন টাস্ক। এটাকে ক্লাসিফিকেশন টাস্ক হিশেবে ভাবা যেতে পারে।
\subsubsection{n গ্রামস}
\subsubsection{ল্যাঙ্গুয়েজ মডেল এভালুয়েশন}
\subsubsection{স্মুদিং}

\subsection{রিকারেন্ট নিউরাল নেটস}
রিকারেন্ট নিউরাল নেটওয়ার্কে \cite{6302929} এক ধরণের চেইন থাকে। এটাকে ফিডব্যাক লুপ বলা হয়। চেইন বা সিকুয়েন্সিয়াল বিধায় এই ধরণের নেটয়ার্ক সিকুয়েন্স নিয়ে ভালো কাজ করতে পারে। 
আরএনএন (RNN) এবং মাল্টিলেয়ার পার্সেপ্টন মূলত একই রকম, একমাত্র পার্থক্য হলো আরএনএন এর হিড্ডেন ইউনিটসগুলির মধ্যে কানেকশন রয়েছে।
চিন্তা করা যাক, একটা টেবিলের উপর অনেকগুলি নিউরন একটার একটা এক সারিতে বসানো হলো। এটা হলো আমাদের মাল্টিলেয়ার পার্সেপ্টন।
এই নিউরনগুলির আগেরটির সাথে যদি পরেরটাকে জোড়া দেয়া হয় তবে সেটি হবে রিকারেন্ট নেটওয়ার্ক। 
সাধারণত প্রতি টাইম স্টেপে মডেল আগের টাইম স্টেপের হিড্ডেন স্টেটকে ইনপুট হিশেব নেয়। নেটওয়ার্কে ইনপুট $x$, হিড্ডেন স্টেট $h$ এবং আউটপুট $y$। 
রিকারেন্ট নেটওয়ার্কে টাইম স্টেপ  $t-1$ থেকে $t$ একটা কানেকশন থাকবে। কানেকশন থাকা বলতে বুঝাচ্ছি $t-1$ এর হিড্ডেন ষ্টেট $h_{t-1}$ পরের স্টেপ $t$ এর ইনপুট। 

\begin{align} % requires amsmath; align* for no eq. number
  ￼￼h_t = F(h_{t−1}, \ x_t, \ θ)\\
  h_t = W_{rec} \sigma (h_{t−1}) + U x_t + b \\ 
  y_t = F(V h_t)
\end{align}

নেটওয়ার্ক প্যারামিটার $U, W$ ও $V$ শেয়ারড প্রতি স্টেপে। প্যারামিটার $W$ রিকারেন্ট। ফিডব্যাক লুপের এই প্রোপার্টি ন্যাচারাল ল্যাঙ্গুয়েজের জন্য বেশ কাজের। 
সাধারণত এইযে আমি এখন লিখছি একটা শব্দের পর আরেকটা শব্দের গাঁথুনি হচ্ছে। দেয়াল বানাতে যেমন একটা পর একটা ইটের গাঁথুনি দেয়। 
আমার আগের শব্দ যা লিখলাম তার উপর নির্ভর করে পরের শব্দটী লিখবো। এটা কিন্তু র‍্যান্ডম না। এখানে আগের শব্দের বা শব্দগুচ্ছের (prefix)  একটা গুরুত্বপুর্ণ প্রভাব থাকবে। 
ম্যাশিন লার্নিং বিশেষ করে ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিংয়ে  অধিকাংশ ড্যাটা সিকুয়েন্সিয়াল। টেক্সট, অডিও, ভিডিও ইত্যাদি সবকটা সিকুয়েন্সিয়াল।
রিকারেন্ট নেট এই সিকুয়েন্স ইনফর্মেশনের মাধ্যমে ভালো কন্টেক্সট সংগ্রহ করতে পারে। এই সিকুয়েন্স অর্ডার যদি নেটওয়ার্কে আমরা ইনপুট হিশেবে না পাঠাতে পারি তাহলে 
মডেল ট্রেইন করা সম্ভব হবে না অধিকাংশ ক্ষেত্রে। আশা করি কেন রিকারেন্ট নেটওয়ার্ক আর্কিটেকচার হিশেবে আমরা নির্বাচন করছি তার একটা ব্যাখ্যা দিতে পারলাম। 

\subsection{ট্রেইনিং রিকারেন্ট নিউরাল নেটস}
এবার আমরা নেটওয়ার্ক ট্রেইনিং এর দিকে নজর দিব। এই নেটয়ার্ক আমরা কিভাবে ট্রেইন করতে পারি? এটা অন্যান্য নিউরাল নেট ট্রেইনিং থেকে আলাদা কিছু নয়।
এখানেও আমরা স্টকাস্টিক গ্র্যাডিয়েন্ট ডিসচেন্ট  (SGD) ব্যবহার করবো। রিকারেন্ট নিউরাল নেটের লস হলো প্রতি স্টেপের লসের যোগফল। 
\begin{align} % requires amsmath; align* for no eq. number
   E = \sum_{t=0} ^{T} E_t \\
   \frac {\partial E} {\partial U} = \sum _{t=0} ^{T} \frac {\partial E_t} { \partial U}
\end{align}



\subsubsection{ব্যাকপ্রোপাগেশন থ্রু টাইম (বিপিটিটি)}
আমরা ব্যাকপ্রোপাগেশন ব্যাবহার করবো নিউরাল নেটওয়ার্ক ট্রেইন করার জন্য। বুঝার সুবিধার্থে আমরা এই লস ফাংশন নিয়ে বিস্তারিত চিন্তা করবো এখন। আমরা যদি একটা স্টেপ নিই, $t$ থেকে $t+1$ পজিশনে যাই। তাহলে আমাদের লস ফাংশনের $E$ ডেরিভেটিভ ইনপুট $U$ এর সাপেক্ষে দাঁড়ায়। 

\begin{align} % requires amsmath; align* for no eq. number
   \frac {\partial E_{t+1}} {\partial U} = \frac {\partial E_{t+1}} { \partial h_{t+1}} \frac {\partial h_{t+1}} { \partial U} = \frac {\partial E_{t+1}} { \partial h_{t+1}} \frac {\partial h_{t+1}} { \partial h_t} \frac {\partial h_t} { \partial U}\\
\end{align} 

ব্যাকপ্রোপাগেশন থ্রু টাইম হবে - 
\begin{align} % requires amsmath; align* for no eq. number
   \frac {\partial E_{t+1}} {\partial U} = \sum _{k=0} ^{t+1} \frac {\partial E_{t+1}} { \partial h_{t+1}} \frac {\partial h_{t+1}} { \partial h_k} \frac {\partial h_k} { \partial U} \\
\end{align}



\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=\textwidth]{images/lstm_error.pdf} 
   \caption{রিকারেন্ট নেট ইরর}
   \label{fig:rnn_error}
\end{figure} 


এখানে একটা ট্রিক আমরা ইউজ করবো যার সংক্ষিপ্ত নাম বিপিটিটি \cite{58337}। আমরা যদি লম্বা একটা সিকুয়েন্সের উপর ব্যাকপোপাগেশন করতে যাই সেটা মেমোরি এক্সপেন্সিভ এবং কম্পিউটেশনালি স্লো হতে পারে।
তাই আমরা একটা এপ্রোক্সিমেশন ব্যবহার করবো। 


\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=\textwidth]{images/rnn_derivative.pdf} 
   \caption{রিকারেন্ট নেট ব্যাকপ্রোপাগেশন থ্রু টাইম}
   \label{fig:rnn_derivative}
\end{figure} 

\subsubsection{টিচার ফোর্চিং}
\subsubsection{লং টার্ম ডিপেন্ডেন্সি}
আমরা দেখতে পাচ্ছি রিকারেন্ট নেটের টাইম স্টেপ $t$ তার আগের স্টেপ $t-1$ এর উপর নির্ভরশীল। তাহলে আমাদের সিকুয়েন্স যদি ১০০ টা স্টেপ থাকে তবে সর্বশেষ ধাপ ১০০, তার আগের ধাপ 
৯৯ এর উপর নির্ভরশীল। এখানে আমরা রিকারেন্ট ব্যাপারটা খেয়াল করতে পারি। কারণ ৯৯ তম ধাপ তার আগের ধাপ ৯৮ এর উপর নির্ভরশীল। এভাবে করে ৯৮ ধাপ ৯৭ এর উপর নির্ভশীল এবং 
সবার শেষের ০ তে এসে থামবে। লক্ষ্য করা যাক, ১০০ তম ধাপ চেইন ধরে ৯৯ থেকে ০ সবগুলি ধাপের উপরও নির্ভর করছে। কারণ একটাকে কম্পিউট করতে গেলে তার আগেরটার ইনফরমেশন ছাড়া 
কম্পিউটেশন করা সম্ভব না। এই ধরণের ডিপেন্ডেন্সিকে বলা হয় লং টার্ম ডিপেন্ডেন্সি। 

\subsection{এক্সপ্লোডিং ও ভ্যানিশিং গ্র্যাডিয়েন্ট}
রিকারেন্ট নিউরাল নেট ভ্যানিশিং গ্র্যাডেইয়েন্ট প্রব্লেম হয়। সহজ কথায় বললে, মডেল একটা লম্বা সিকুয়েন্স নিয়ে কাজ করতে গেলে আগে যেসব ইনপুট পেয়েছিল সেসব ভুলে যায়।
ভ্যানিশিং গ্র্যাডিয়েন্টের জন্য আমাদেরকে জ্যাকোভিয়ান টার্মটা বুঝতে হবে। 

\subsubsection{জ্যাকোভিয়ান}
\subsubsection{গ্র্যাডিয়েন্ট ক্লিপিং}

\subsection{লং শর্ট টার্ম মেমোরি নেটওয়ার্ক}
আরএনএন এর ভ্যানিশিং গ্র্যাডিয়েন্ট সমস্যা সমাধান করার জন্য লং শর্ট টার্ম মেমোরি নেটোওয়ার্ক ব্যবহার করা হয়। 
\subsubsection{মেমোরি সেল}
যেকোন কম্পিউটেশনাল মেমোরিতে দুই ধরণের অপারেশন সম্ভবঃ ১) রিড ও ২) রাইট। যদি প্রথমবার লিখা হয় সেটাকে বলা হয় ইনসার্ট আর যদি পুরানো লিখা মুছে (ভুলে গিয়ে) নতুন কিছু লিখা হয় সেটাকে বলা হয় আপডেট। 
 
\subsubsection{ইনপুট, ফরগেট ও আউটপুট গেট}
এলএসটীএম মেমোরি সেলে তথ্য আপডেট করার জন্য ৩ টী গেট ব্যবহার করা হয়। এখানে আমরা ধাপে ধাপে ৩ টী গেটের কাজ বুঝার চেষ্টা করবো। 
 
\begin{align} % requires amsmath; align* for no eq. number
   f_t = \sigma \ (W_f \cdot [H_{t-1} , x_t] \ + \ b_f) \\
   i_t = \sigma \ (W_i \cdot [H_{t-1} , x_t] \ + \ b_i) \\
   \tilde C_t = \tanh \ (W_c \cdot [H_{t-1} , x_t] \ + \ b_c) \\
   C_t = f_t * C_{t-1} + i_t * \tilde C_t \\
   o_t = \sigma \ (W_o \cdot [H_{t-1} , x_t] \ + \ b_o) \\ 
   h_t = o_t * \tanh(C_t) \\ 
\end{align}

\subsection{দুটি অত্যাবশ্যকীয় উপাদানঃ নিউরাল এম্বেডিংস + রিকারেন্ট ল্যাঙ্গুয়েজ মডেল}
আমরা যদি সিকুয়েন্সিয়াল ড্যাটা নিয়ে কাজ করতে চাই তাহলে আমাদের এখন দুটি উপাদান লাগবে- ১) ওয়ার্ড ভেক্টরস বা নিউরাল এমবেডীংস ও ২ ) রিকারেন্ট নিউরাল নেটওয়ার্ক। 
আর এই নেটওয়ার্ক ট্রেইন করতে আমরা ব্যবহার করবো বিপিটিটি এলগরিদম। ওয়ার্ড ভেক্টরস আমাদের ফিচার আর রিকারেন্ট নেট আমাদের ডিপ লার্নিং টুল। 
সাধারণত আমরা এলএসটিএম (LSTM) ব্যবহার করার চেষ্টা করবো কারণ ওর লং টার্ম ডিপেন্ডেন্সি নিয়ে কাজ করার সক্ষমতা বেশি। গ্র্যাডিয়েন্ট ইস্যুর কারণে রিকারেন্ট নেট প্র্যাক্টিক্যালি ট্রেইন করা 
কষ্টসাধ্য কাজ। মোটাদাগে, আধুনিক ভাষা প্রযুক্তির সমস্যা সমাধানে এই দুই অস্ত্র থাকলেই কাজ চালিয়ে নেয়া যাবে! 

\textit{কিছু টিপস\#}  সিকুয়েন্সিয়াল ইনপুট আউটপুট বিষয়ক কিছু সূক্ষ্ম বিষয় আছে যা খেয়াল রাখা জরুরি। আমরা যখন কোড লিখবো বা মডেল পার্ফর্মেন্স এনালাইসিস করবো তখন এই বিষয়গুলি কাজে দিবে।
সিকুয়েন্স মডেলিংয়ের কাজ করতে গেলে আমাদের নিচের কয়েকটি বিষয় মাথায় রাখতে হবে। 

\begin{enumerate}[label=(\roman*)]
\item ভ্যারিয়েবল লেংথ সিকুয়েন্স নিয়ে কাজ করতে হতে পারে (যেকোন টেক্সট ইনপুট;  অথচ ছবি চাইলে আমরা ফিক্সড ডাইমেশন রূপান্তর করে নিতে পারি) 
\item শব্দের অর্ডার মনে রাখা (ভাষার বাক্য গঠন বৈশিষ্ট্য) 
\item লং টার্ম ডিপেন্ডেন্সি মনে রাখা (সমৃদ্ধ কন্টেক্সট এর জন্য দরকার) 
\item সিকুয়েন্সগুলির মধ্যে প্যারামিটার (নেটওয়ার্ক ওয়েটস) শেয়ার করা (ট্রেইনিং এফিশিয়েন্সির জন্য; প্রতি স্টেপে আলাদা প্যারামিটার থাকলে প্যারামিটারের সংখ্যা লিনিয়ারলি বেড়ে যাবে!) 

\end{enumerate}

\section{ম্যাশিন ট্রান্সলেশন ও  সিকুয়েন্স টু সিকুয়েন্স মডেল}
\subsection{ম্যাশিন ট্রান্সলেশন টাস্ক}
\subsection{স্ট্যাটিস্টিক্যাল ম্যাশিন ট্রান্সলেশন} 
স্ট্যাটিস্টিক্যাল ম্যাশিন ট্রান্সলেশনের ৩ টি কম্পোনেন্টঃ ট্রানস্লেশন মডেল, ল্যাঙ্গুয়েজ মডেল ও  ডিকোডিং। 

\subsection{সিকুয়েন্স টু সিকুয়েন্স মডেল} 
\subsubsection{এনকোডার ডিকোডার}
আমরা যদি বাংলায় যদি একটা বাক্যকে অনুবাদ করতে চাই তাহলে আমাদের বাংলা বাক্যটি হবে ইনপুট আর ইংরেজি বাক্যটি পাবো আউটপুটে। 
ইনপুট বাক্যকে নিবে এনকোডার। আর এনকোডার এর ইনপুট থেকে আউটপুট বাক্য দিবে ডিকোডার। আমরা এনকোডার ডিকোডার দুটোর জন্য রিকারেন্ট নিউরাল নেট ব্যবহার করতে পারি।
আমাদের একটা আরএনএন (RNN)  লাগবে এনকোডারের জন্য আরেকটা লাগবে আউটপুট ডিকোড করতে। 
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=\textwidth]{images/seq2seq_ilya.pdf} 
   \caption{এনকোডার ডিকোডার মডেল }
   \label{fig:seq2seq}
\end{figure} 


\section{এটেনশন মেকানিজম}
সিকুয়েন্স টু সিকুয়েন্স মডেলে একটি বটলনেক রয়েছে। রিকারেন্ট নেটের সর্বশেষ হিড্ডেন ষ্টেট নিয়ে কেবল কাজ করতে পারবো আমরা। এর মানে আমাদের সকল ইনফর্মেশন একটা জায়গায় ফিক্সড।
এই লাস্ট হিড্ডেন স্টেটকে কন্টেক্সট ভেক্টর বলা হয়ে থাকে। ফিক্সড ভেক্টর ব্যবহার করলে নেটোয়ার্ক লং টার্ম ডিপেন্ডেন্সি নিয়ে কাজ করতে পারে না।
 সেই সমস্যা সমাধানে এটেনশন মেকানিজম ইউজ করা হয়। এমন যদি হয় যে ডিকোডার একটা ফিক্সড কন্টেক্সট না নিয়ে প্রতি টাইম স্টেপে এনকোডারের পুরো সিকুয়েন্স নিয়ে কাজ করতে পারবে?
 পুরো সিকুয়েন্সে যদি এনকোডার এক্সেস থাকে তাহলে এনকোডার যে স্টেটটি বেশি গুরুত্বপূর্ণ সেটিকে ব্যবহার করতে পারবে।  
 এটেনশন ব্যবহার করলে এনকোডারের সবগুলি সিকুয়েন্সের উপর কাজ করা যায় এবং ইনপুটের সবচে উপযোগী অংশ মডেল ব্যবহার করতে পারে। 

\subsection{বাহদান এটেনশন}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.4\textwidth]{images/attention_bahdanau.pdf} 
   \caption{এটেনশন মেকানিজম}
   \label{fig:attn_bahdanau}
\end{figure} 

\subsection{মাল্টি হেড এটেনশন}

\subsection{সেলফ এটেশন}
\subsection{সেক২সেক এপ্লিকেশন}
\subsubsection{নিউরাল ইমেজ ক্যাপশনিং}
\subsection{ট্রান্সফর্মার}
\subsection{সেলফ এটেনশন ও কনভল্যুশনের মধ্যকার সম্পর্ক}

\section{রিকার্সিভ ও কনভল্যুশনাল নিউরাল নেটস ফর ল্যাঙ্গুয়েজ প্রসেসিং}
\subsection{রিকার্সিভ নিউরাল নেটস}
\subsection{কনভল্যুশন}
কনভল্যুশন হলো এক ধরণের ফিচার একস্ট্র্যাকটর। কনভল্যুশন অপারেশনের মাধ্যমে আমরা ইনপুট টেক্সটের লোকাল ফিচারগুলি খুঁজে বের করতে পারি। 
কনভল্যুশন এক ধরণের গাণিতিক অপারেশন যার কাজ হল ইনপুট ম্যাট্রিক্সের উপর স্লাইডিং উইন্ডো ফাংশন এপ্লাই করা।
স্লাইডিং উইন্ডো ফাংশনটীর অনেক নাম, কার্নেল, ফিল্টার বা ফিচার এক্সট্র্যাক্টর। কনভল্যুশন বুঝতে গেলে আমাদের উইন্ডো সাইজের বিষয়টি খেয়াল করতে হবে। 
যেটা হচ্ছে আমরা একটা ফিল্টার নিচ্ছি (ধরা যাক ৩x৩, কার্নেল সাইজ = ৩) এবং পুরো ছবির উপর স্লাইড করছি।
একবার স্লাইড করলে আমরা ইনপুট ম্যাট্রিক্সের উপর একবার স্ট্যাম্প করবো। এখন আমাদের ইনপুটের একটা ব্লক আছে আর ফিল্টার আছে যা ইনপুটের ব্লকের উপর। 
এখানে আমরা এলিমেন্ট ওয়াইজ ম্যাট্রিক্স মাল্টিপ্লিকেশন করে তারপর যোগ করে নিবো।



\subsection{কনভল্যুশন ফর সেনটেন্স মডেলিং}
টেক্সটের জন্য সাধারণত 1D কনভল্যুশন ব্যবহার করা হয় \cite{kalchbrenner-etal-2014-convolutional}।  ধরা যাক, আমাদের কাছে একটি বাক্য রয়েছে।  আমরা উইন্ডো সাইজ নিলাম ২। 1D কার্নেলটি হবে (১x3)!  
এখন আমরা এই দুই উইন্ডো সাইজ নিয়ে টেক্সটের উপর স্লাইড করতে পারি। প্রতিটা স্লাইডিং স্টেপে আমরা একটা নিউমারিক্যাল ভ্যালু পাবে। 
এভাবে পুরো বাক্য স্লাইড করা শেষ হলে আমরা একটা ফিচার পাবো যেটা কো-অকারেন্স ইনফর্মেশন সংগ্রহ করেছে। 
এখন যদি আমরা অনেকগুলি ফিচার ম্যাপ ব্যবহার করে তাহলে আমরা নানান রকম ফিচার এক্সট্র্যাক্ট করতে পারবো। 

মজার বিষয়,  আমরা চাইলে যেকোন উইন্ডো সাইজ ব্যবহার করতে পারি। সাধারণত উইন্ডো সাইজ হতে পারে ১,২,৩। 
এখানে লক্ষ্য করলে বুঝা যাবে আমরা যখন স্লাইড করছি তখন কো-অকারেন্স কম্পিউটেশন হচ্ছে। মানে আমরা সেন্টার পয়েন্টের আশেপাশে কোন শব্দগুলি আছে সেগুলিকে ফিল্টার ম্যাপের আওতাও বিবেচনা করছি।
ধরা যাক, ৩x৩ ফিল্টার সাইজ, সেন্টার ওয়ার্ড হলো মাঝখানে শব্দটি আর ডান বাঁয়ে দুটি শব্দ আমাদের সেন্টার ওয়ার্ডের প্রতিবেশী শব্দ। আমরা যদি এক ধাপ স্লাইড করি তাহলে সেন্টার ওয়ার্ডটি 
তখন পরের ধাপে হয়ে যাবে প্রতিবেশী শব্দ। কনভল্যুশন কো-অকারেন্সের উপর নির্ভর করে আমাদেরকে ভালো স্প্যাশিয়াল ফিচার দিবে। মানে কোন শব্দ কার আশেপাশে বসতে পারে এই ইনফর্মেশন 
মডেল শিখবে ইনপুট থেকে। উদাহরণ বাক্য, আমার মন ভালো। মন শব্দের পর সাধারণত "ভালো",  "খারাপ" এই শব্দগুলি নিয়মিত বসে। আমরা যদি অনেক ইনপুট নিয়ে কাজ করি মডেল এই কো-অকারেন্স 
বৈশিষ্ট্য শিখে নিতে পারবে। যদি এটা মডেল বুঝতে পারে তাহলে সে বাক্যের গঠন সম্পর্কে এক ধরণের লার্নিং পাচ্ছে যা ফিচার হিশেবে গুরুত্বপূর্ণ।

\subsubsection{n-গ্রাম ও 1D কনভল্যুশন}
মন দিয়ে লক্ষ করলে বুঝা যাবে, কনভল্যুশন অপারেশনের সাথে n-gram মডেলের সাদৃশ্য আছে। 
লক্ষ্য করি, ফিল্টার সাইজ ২ হলে এটা একটা ২ গ্রাম মডেল কেননা মডেল ঠিক বাইগ্রামের মতন করে ফিচার তইরি করবে। 
আমরা কনভনেটকে n-গ্রাম ফিচার এক্সট্যাক্টর হিশেবে চিন্তা করতে পারি। এখন আমাদের পরের ধাপে কাজ হবে ফিচারগুলি জোড়া দিয়ে নেটওয়ার্ক ট্রেইন করা।

কনভল্যুশনকে ন্যাচারাল ল্যাংগুয়েজ প্রোসেসিংয়ে phrase বেইজড মডেল হিশেবে ব্যবহার করা হয়। রিকারেন্ট নেটে সিকুয়েন্সিয়াল ডিপেন্ডেন্সি থাকে। 
রিকারেন্ট নেট prefix নিয়ে কাজ করে আর কনভল্যুশন নেটওয়ার্ক n-gram phrase নিয়ে কাজ করে।
একটা বাক্য রিকারেন্ট নেটওয়ার্ক প্রসেস করবে সিকুয়েন্সিয়াল ডিপেন্ডেন্সি তইরি করে আর কনভল্যুশন নেটওয়ার্ক অনেকগুলি phrase থেকে ফিচার নিবে। 
কনভল্যুশন নেটোয়ার্কের ইনপুট দিবো ওয়ার্ড ভেক্টরস। আমরা অনেকগুলি ফিল্টার ব্যবহার করতে পারি। সাধারণত ফিল্টার সাইজ ২,৩,৪,৫ হয়ে থাকে। 
লক্ষ্য করি, ফিল্টার সাইজ ২ হলে এটা একটা ২ গ্রাম মডেল কেননা মডেল ঠিক বাইগ্রামের মতন করে ফিচার তইরি করবে। 
আমরা কনভনেটকে n-গ্রাম ফিচার এক্সট্যাক্টর হিশেবে চিন্তা করতে পারি। 

\subsubsection{পুলিং ও ডিপ স্ট্যাকিং}
এখন প্রশ্ন হচ্ছে কনভল্যুশন লেয়ারে অনেকগুলি ফিচার আমরা পাবো। সবগুলি ফিচার কি আমাদের কাজে লাগবে? 
উত্তর হচ্ছে, না! এইজন্য পরের লেয়ারে আমরা একটি পুলিং অপেরেশন করবো। পুলিং অপারেশনের কাজ হলো সবচে ভালো ফিচার ভেক্টরগুলি খুঁজে বের করা। 
তারপর আমরা একটি সফটম্যাক্স লেয়ার ব্যবহার করে ক্লাসিফিকেশন কাজ করতে পারি। আমরা চাইলে কনভল্যুশন + পুলিং একটা পর একটা স্ট্যাক করতে পারি। 
ডিপ নিউরাল নেটওয়ার্ক স্ট্যাকিং ভালো ক্লাসিফিকেশন পার্ফমেন্স দেয়।

\subsection{কনভল্যুশন ফর সেনটেন্স ক্লাসিফিকেশন}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.8\textwidth]{images/cnn_full_zhangetal_2015.png} 
   \caption{সেন্টেন্স ক্লাসিফিকেশনের জন্য কনভল্যুশন নিউরাল নেটওয়ার্ক আর্কিটেকচার ইলাস্ট্রেশন}
   \label{fig:cnn_zhangetal} 
\end{figure} 

\subsection{কনভনেটস ট্রেইনিংঃ  কিছু প্র্যাক্টিক্যাল ট্রিকস}

\subsection{রিকারেন্ট ও কনভল্যুশনাল নিউরাল নেটওয়ার্কের তুলনামূলক আলোচনা}

\section{প্রিটেইন্ড ল্যাঙ্গুয়েজ মডেলস}
\subsection{কনটেক্সুলাইজড রিপ্রেজেন্টেশন্স}
যদি আমরা ওয়ার্ড২ভেকে ওয়ার্ড অর্ডার রাখতে পারি তাহলে কি রিপ্রেজেন্টেশন আরো ভালো হবে না? আমরা অনেক ভালো সিমান্টিকস এবং সিনট্যাক্স 
ইনফরমেশন ম্যাশিনকে দিতে পারবো। 

\subsection{প্রি-টেইনিং}

\subsection{বার্ট (BERT)}

\subsection{জিপিটি ৩ (GPT 3)}


 \part{পার্ট ৪} 

\section{GLUE বেঞ্চমার্ক টাস্কস}
গ্লু বেঞ্চমার্ক \cite{wang-etal-2018-glue}

\subsection{সেন্টেন্স ক্লাসিফিকেশন}
\subsection{ন্যাচারাল ল্যাঙ্গুয়েজ  ইনফারেন্স}
\subsection{কোশ্চেন আনসারিং}


\section{ট্রান্সফার লার্নিং ফর ল্যাঙ্গুয়েজ প্রসেসিং}
অনেক ডিপ লার্নিং সমস্যা সমাধানে যথেষ্ট ড্যাটা পাওয়া যায় না। কিন্তু অন্য কোন সমস্যা সমাধান করতে গিয়ে আমরা যা শিখেছি সেটা আমরা কাজে লাগাতে পারি। 
এটাকে বলা হয় ট্রান্সফার লার্নিং। শব্দ দুটি দেখেই বুঝা যাচ্ছে লার্নিং কে আরেকটা কাজে ট্রান্সফার করা হবে। 
প্রিটেইন্ড ল্যাঙ্গুয়েজ মডেলের জন্য ট্রান্সফার লার্নিং খুবই নিয়মিত এখন। 

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.6\textwidth]{images/transfer_learning_scenario.png} 
   \caption{ট্রান্সফার লার্নিং}
   \label{fig:ruder_transfer_learning}
\end{figure} 

\subsection{সিকুয়েন্সিয়াল ট্রান্সফার}
\subsection{মাল্টি-টাস্ক ট্রান্সফার লার্নিং}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=0.6\textwidth]{images/mtl_images_ruder.png} 
   \caption{মাল্টি টাস্ক লার্নিং}
   \label{fig:ruder_mtl}
\end{figure} 
\subsection{লাইট ওয়েট ফাইন-টিউনিং}

\section{এথিকস ও ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং}
\subsection{জেন্ডার বায়াস ও ওয়ার্ড  এম্বেডিং}
\subsection{ডিবায়াসিং}
\subsection{স্টকাস্টিক প্যারটস}

\section{ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং রিসার্চ}
\subsection{থিম ১ঃ ল্যাঙ্গুয়েজ মডেলিং}
\subsection{থিম ২ঃ কমনসেন্স রিজনিং}
\subsection{থিম ২ঃ মডেল এনালাইসিস ও এক্সপ্লেনেশন}
\subsection{থিম ৩ঃ  মাল্টিলিঙ্গুয়াল ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং} 
\subsection{থিম ৪ঃ কন্টিনিউয়াল লার্নিং}


\bibliographystyle{unsrt}
\bibliography{ref}
\end{document}